---
title: 54th Conference of the ACL
layout: blogposts
tags:
 - NLP
 - ACL
imgsrc: /graphics/illustrations/2016-08-11/humboldt.jpg
imgalt: Humboldt University's main building in Berlin.
imgcaption: Humboldt University's main building in Berlin.

venue: 
authors: Olof Mogren
permalink:
pdf: 
overwriteurl: 
published: false
---

August 7-12, the 54th conference of the Association of Computational Linguistics (ACL) took place at the Humboldt University in Berlin, along with co-located tutorials and workshops. The conference was attended by roughly 1700 people, and hundreds of papers were presented during the sessions.

Also see my posts about the [Tutorial on Neural Machine Translation](http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html) and the [First Workshop on Representation Learning for NLP](http://mogren.one/blog/2016/08/11/representation-learning-for-nlp.html).

## Monday

(Perhaps change headings to be more topically related).

**Generating Factoid Questions with RNNs**, Iulian Vlad Serban; Alberto García-Durán; Caglar Gulcehre; Sungjin Ahn; Sarath Chandar; Aaron Courville; Yoshua Bengio

This paper presents a large corpus of 30 million question-answer pairs, design to use in the development of QA systems. The data is synthetically created using factoid data from Freebase and a neural network architechture. The generated questions is evaluated and compared to questions generated by a template-based method. 

* Transducing KB facts into questions.
* Motivation: generate synthetic QA dataset to bootstrap QA systems.
* Crowdsourced dataset.
* Seq2seq
* Sparsity! Most entities occur only once in the whole dataset.
* How do we learn embeddings? TransE: an energy based model for multi-relational embeddings. (SOURCE SIDE)
* Entity place holders for TARGET SIDE.
* QA dataset: agarciaduran.org
* Nice presentation, interesting problem and somewhat creative solution.
* Will not release source code.

**Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models**, Minh-Thang Luong and Christopher D. Manning

This paper presents a novel NMT model where out-of-vocabulary words are
treated as a character sequence. They report improvement in BLEU scores
over models that handle OOV words in some way, and claim that this
model produces translations with better quality of the OOV translations.

When the system comes across an OOV word, it uses a trained deep LSTM
on character level. This network is always initialized with a zero state.
The final internal state is used as the representation for this word.
The reason for the zero initialization is to make the system simple;
the representation of all instances of one OOV word can be precomputed 
before using the
word-based model.

When the word-based decoder produces an <UNK>, the character based
decoder is used. 

* How does this compare
  to the subtoken model presented by Bengio et.al.? The authors mention
  one paper doing this by Sennrich 2016, and claim that they do not
  manage to learn "relationships among words".?
* How is the character based model trained? Is there a regularization,
  or even main training objective that makes the representations
  similar between the two models?
  - NO. (asked Than Luong in person)
* Byte-pair algorithm starts with a one-character vocabulary, and then adds
  the most frequent two-gram to the vocabulary. 

**Monolingual Data to improve NMT**, Sennrich, Haddow, Birch

* The encoder-decoder is already a language model.
* Can we feed it monolingual data?
* Approximating the source sidel.
  - Dummy word inputs on source side.
    + Too much training of this makes the model disregard the source side,
      and rely to heavily on the target side.
  - Produce synthetic source data by back-translation.
* The first approach does not help much. The second approach gives improvements of up to three BLEU points.

**Siamese neural networks for QA systems**

* Ranking similar questions.
* Construct similar representations for questions and answers.
* Contrastive loss function.



## Tuesday

**neural machine translation of rare words with sub word units**, Sennrich

"character level model working on compressed character data"

question: Minh-thang luong: cannot capture relations between words. ("Sennrich et al. (2016) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.")  comment?

comment: there is nothing saying that word boundaries are the best way to split the symbols. a compound word in one language can be translated into several words in another language. and it works well.

**Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change**, William Hamilton, Jure Leskovec and Dan Jurfsky

* Diachronic (historica) linguistics
* Qualitatively tendencies: "Grammaticalization" over time.
* Quantitatively tendencies:
* frequency, polysemy (factors that change over time)
* 4 languages
* Many embeddings.
* No evaluation of diachronic properties.
* Why do some words cahnge faster than others?
  - Mixed effect analysis.
* 5% of sentiment words changed polarity during last decade(s?)

**A Character-Level Decoder without Eplicit Segmentation for Neural Machine Translation**, Junyoung Chung, Kyunghyun Cho, Yoshua Bengio

This paper presents a model trained using subword-units as inputs and a
character sequence as output. The input is in the form of "byte-pairs"
as in Sennrich et.al. 2015.

There is talk about an ensamble, but I have not yet understood what it is.

The character-based decoder outperforms a decoder with subword-units
(presumably byte-pairs here too).

Traditionally, translation systems work on word-level, even though a neural
model can suffer from large vocabulary sizes. This is why we see a number
of systems that try to remedy this, initially by subword-level modelling,
but now also on character-level.

**Entity-focussed way to generate company descriptions**, 

* *Perhaps this one is a bit too template-based to be featured by me, but the presentation was nice*.
* Template-filling.
* Lots of templates. Then cluster the resulting sentences and finally show the best ones.
* A second system is more data-driven.
* After this, a hybrid approach makes the final description.
* Simple solution, good presentation, but perhaps not extremely interesting solution.

## Wednesday

**ACL's Lifetime achivement award 2016** was given to linguistics professor Joan Bresnan of Stanford who gave a nice acceptance speech about her transition from formal grammars into the wild world of unstructured data.

**On-line active reward learning for policy optimization in spoken dialogue systems** (outstanding paper), Su, Gasic, Mrksic, Barahona, Ultes, Vandyke, Wen, Young

* Pretty cool RL solution!
* Task-oriented dialogue system
* Success evaluation
* Goal is to find a suitable training system.
* Statistical spoken dialogue systems
* slide-photo.
* Per-turn penalty: -1
* Large reward if completion is successfulTypically requires pror knowledge of the task.
  - Simulated user
  - Paid users mturk
  x real users (have their own goal)
* How to learn from real users?
* Infer success (reward) directly from dialogues.
  - Train a reward estimator from data.
* User rating. (Noisy, difficult/costly to obtain)
* Propose a robust reward model on user rating.
* Learning a reward model.
  - Embedding function. Fixed length dialogue representation.
  - The leared reward model will then be used with reinforcement learning.
  - BLSTM encoder-decoder. As an autoencoder!
  - Mean squared error in reconstruction.
  - Gaussian process for success rating. Measurement of the uncertainty.
    + RBF kernel. Noise term. Affects the uncertainty. More noise -> less certain predictions.
  - ( I think that the gaussian process selects training examples?)
* Then the reward system is used in the SDS.
* Off-line RNN, Subj, Online GP
* All reached > 85% after 500 dialogues.
* Onlie GP is more robust than subj.
* Online GP only needs 150 dialogues forsomething.
* Conclusion slide on photo.
* Pretty cool!


**Thorough examination of CNN/Daily Mail reading comprehention task** (outstanding paper), Danqi Chen, Jason Bolton, Chris Manning

Comments:

* *Nice to implement a strong baseline. Both methods were actually interesting.*
* *Inspiring analysis, but did not follow entirely in the end; what are the differenct categories?*

* Reading comprehention data: need more
* In the CNN/Daily Mail, each article have a number of bullet points. Pick one, and black out one word (an entity, I think). Problem will be to predict the word.
* System 1: entity centric classifier
  - a symbolic feature vector for each entity.
* System 2: end-to-end neural system.
  - GRU
* Difference to attentive reader:
  - Bilinear attention
  - Remove redundant layer efore prediction
  - Predict among entities only, not all words.
* Conclusions:
  - Simple models sometimes just work. Neural nets are great for learning semantic matches.
  - The dataset is noisy and we have almost hst the capcacity: not hard enough for reasoning an dinference.
  - Future:
    + Leverage these datasets to solve more realistic RC tasks.
    + Complex models?
    + More datasets coming up: WikiReading, LAMBADA SQuAD


**Learning language games through interaction** (outstanding paper), Sida Wang, Percy Liang, Chris Manning

* *Pretty entertaining! Nice presentation*
* Wish list:
  - Interactive learning, adapt to users, handle special domains and low resouce
* Interactive language game.
  - Human player, knows goal, knows language
  - Computer player, does not know the goal, doesnot understaand language
    + need to learn language
* ShrdLURN
  - user writes command.
  - Is given some candidate actions; accepts one.
  - Semantic parsing
    + start from smallest size, score them with a model (log-linear model with features)
    + adagrad
    + features uni, bi, skipgrams, treegrams
* Players on mturk.
* Players adapt, starting to use only one term.
* Learning works fairly well, especially for top players.
* Cooperative communication
* Pragmatics model
  - Pragmatics helps for the top players
* Interactive learning is good for usability.
  - Feedback mechanism -> less likely to be stuck.
  - Should introduce learning to normal usage.
  - Good for low-resource languages
  - Learn from the actual distribution


**Globally normalized transition based neural networks** (outstanding paper), Daniel Andor, Chris Albergi, David Weiss, Aliaksei Severyn, Alessandro Presta, Kruzman Ganchev, Slav Petrov, Michael Collins


* *Google. OK. But did not explain the global normalization that good.*
Related to Parsey McParseface
* Transition based parsing
* Globally ormalized models using the same #params can be more ...
* Locally normalized training:
  - Use an oracle to map gold actions to gold decisions
  - Minibatches
  - Chen, Manning
* How important is lookahead? 1 improves a lot. Then improves less and less as it is increased.
* Beam search with local model improves a bit.

Training with early updates. (Idea from structured perceptron).

Globally normalized with respect to the beam.

Globally normalized model works much better than the local, even with lookahead.

Will see if this can also improve the LSTM model.

Works also for
  - Sentence compression.
  - POS tagging
  - More examples

Backprop with a beam.

Compared to reinforcement-based methods, training of this model is expensive because you need to decode using a beam. 

The idea damages performance for beam size 1. But improves with larger beams.



| President's talk | Amusing at times. Nice overview on 25 years of MT |
