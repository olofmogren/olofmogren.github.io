I" <h1 id="ordlista-f√∂r-representation-learning-for-natural-language">Ordlista f√∂r <em>Representation learning for natural language</em></h1>

<p>Detta √§r en ordlista f√∂r de som saknar teknisk bakgrund f√∂r att underl√§tta
l√§sandet av avhandlingen
<a href="http://mogren.one/phd"><em>Representation learning for natural language</em></a>,
eller f√∂r att h√§nga med lite b√§ttre p√• disputationen.</p>

<p>Ordlistan √§r inte i alfabetisk ordning, utan f√∂rs√∂ker ist√§llet sig p√• att f√∂rklara
saker fr√•n enkla till mer komplexa.</p>

<p>Ordlistan listar f√∂rst den engelska termen, och d√§refter ett f√∂rs√∂k till en svensk √∂vers√§ttning.</p>

<p><a name="nlp"></a></p>
<ul>
  <li><strong>Natural language processingg (NLP)</strong> - <em>datorlingvistik/computational linguistics</em> - studien av hur naturligt spr√•k (skrivet eller talat av m√§nniskor) kan bearbetas av datorer. P√• senare tid s√• har <a href="#machinelearning">maskininl√§rning</a> blivit ett allt viktigare verktyg inom NLP. B√§gge √§r viktiga delar av  artificiell intelligens.</li>
</ul>

<p><a name="machinelearning"></a></p>
<ul>
  <li><strong>Machine learning</strong> - <em>maskininl√§rning</em> - ett datorprogram som utf√∂r en uppgift och som anv√§nder <a href="#trainingdata">data</a> f√∂r att bli b√§ttre p√• uppgiften. Genom att j√§mf√∂ra det som kommer ut fr√•n programmet med det man ser i datan med hj√§lp av en <a href="#objective">felfunktion</a>, och att f√∂rs√∂ka minimera dess v√§rde, kan programmet bli b√§ttre. En maskininl√§rningsalgoritm kan tr√§nas vid ett tillf√§lle f√∂r att sedan anv√§ndas, eller s√• kan den forts√§tta att f√∂rb√§ttras n√§r den f√•r se ny data.</li>
</ul>

<p><a name="trainingdata"></a></p>
<ul>
  <li><strong>Training data</strong> - <em>tr√§ningsdata</em> - datan som en <a href="#mlmodel">maskininl√§rningsmodell</a> anv√§nder f√∂r att l√§ra sig en uppgift. Exempelvis f√∂r en bildklassificerare, s√• best√•r datan av bilder tillsammans med deras korrekta klassificeringar. Uppgiften blir f√∂r modellen att l√§ra sig att ber√§kna den korrekta klassen utan att titta p√• klassificeringen fr√•n datan.</li>
</ul>

<p><a name="vector"></a></p>
<ul>
  <li><strong>Vector</strong> - <em>vektor</em> - en sekvens av tal. Om vektorns l√§ngd √§r tv√• eller tre, s√• kan dessa tolkas som tv√•dimensionella eller tredimensionella koordinater. √Ñr l√§ngden st√∂rre, s√• kan de ocks√• tolkas som koordinater, fast i ett h√∂gdimensionellt vektorrum. F√∂r att kunna visualisera h√∂gdimensionella vektorer, projicerar man ofta ner dessa till tv√• dimensioner. Detta √§r en process som g√∂r att man f√∂rlorar information fr√•n den ursprungliga vektorn. Vi skriver vektorer med fetstil: <strong><em>v</em></strong>. Exempelvis, en fyrdimensionell vektor: [0.3, 0.8, 0.7, 0.3]. (Notera engelska decimalpunkter. Komma separerar talen i vektorn i detta fall.) (<a href="https://en.wikipedia.org/wiki/Row_and_column_vectors">L√§s mer p√• Wikipedia.</a>)</li>
</ul>

<p><a name="matrix"></a></p>
<ul>
  <li><strong>Matrix</strong> - <em>matris</em> - ett rutn√§t eller tabell av tal. Denna kan ses som en sekvens av <a href="#vector">vektorer</a> (antingen dess rader eller dess kolumner). Vi ben√§mner matriser med stor bokstav: <em>M</em>. Exempelvis, en 4x3-matris:<br />
0.3, 0.5, 0.0<br />
0.8, 0.2, 1.9<br />
0.7, 0.7, 0.0<br />
0.3, 0.1, 1.1<br />
(Notera engelska decimalpunkter. Komma separerar talen i matrisen i detta fall.)<br />
(<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">L√§s mer p√• Wikipedia.</a>)</li>
</ul>

<p><a name="mlmodel"></a></p>
<ul>
  <li><strong>Machine learning model</strong> - <em>maskininl√§rningsmodell</em> - ett sett att strukturera bilden av en maskininl√§rningsalgoritm. Exempel kan vara <a href="ann"><em>neuronn√§t</em></a> eller andra statistiska modeller. Generellt s√• tr√§nas en modell genom att man uppdaterar dess interna parametrar, vilka ofta lagras i <a href="#matrix">matriser</a>.</li>
</ul>

<p><a name="ann"></a></p>
<ul>
  <li><strong>Artificial neural network</strong> - <em>neuronn√§t eller artificiella neurala n√§tverk (svengelska, men vanligt f√∂rekommande)</em> - en klass av <a href="#mlmodel">maskininl√•rningsmodeller</a> som √§r <em>lite</em> inspirerade av hj√§rnan. Ett neuronn√§t byggs med hj√§lp av enkla byggstenar (<a href="#artificialneuron">artificiella neuroner eller units</a>, <a href="#layer">lager</a>, <a href="#activationfunction">aktiveringsfunktioner</a>). Ett neuronn√§t kan approximera vilken given reellv√§rd funktion som helst, bara den har tillr√§ckligt m√•nga <a href="#artificialneuron">neuroner</a>.</li>
</ul>

<p><a name="artificialneuron"></a></p>
<ul>
  <li><strong>Artificial neuron</strong> - <em>artificiell neuron</em> - inspirerad av biologiska neuroner. Dessa moduler (units) tar en <a href="#vector">vektor</a> som input fr√•n f√∂reg√•ende lager (eller fr√•n input-datan), ber√§knar en viktad summa genom att f√∂rst multiplicera input-vektorn <strong><em>x</em></strong> med en vektor av parametrar (eller vikter) <strong><em>w</em></strong>, och adderar en bias <em>b</em>. <strong><em>a</em></strong> = <strong><em>w</em></strong> ¬∑ <strong><em>x</em></strong> + b. Resultatet skickas sen genom en icke-linj√§r <a href="#activationfunction">aktiveringsfunktion</a> f√∂r att f√• det slutgiltiga resultatet fr√•n neuronen..</li>
</ul>

<p><a name="layer"></a></p>
<ul>
  <li><strong>Neural network layer</strong> - <em>lager</em> - ett antal <a href="#artificialneuron">artificiella neuroner</a>, som alla tar en vektor sin indata, och ger ett tal som utdata. Tillsammans, s√• utg√∂r deras utdata en <a href="#vector">vektor</a>, och varje <a href="#layer">layer</a> transformerar allts√• sin indata-vektor till en utdata-vektor.</li>
</ul>

<p><a name="activationfunction"></a></p>
<ul>
  <li><strong>Activation function</strong> - <em>aktiveringsffunktion</em> - en funktion som tar en pre-aktivering <strong><em>a</em></strong> i en <a href="#artificialneuron">artificiell neuron</a> eller <a href="#layer">layer</a>, och applicerar en elementvis icke-linearitet. Det √§r vanligt att man anv√§nder s-formade sigmoidfunktioner s√•som <a href="https://en.wikipedia.org/wiki/Logistic_function">logistiska funktionen (Wikipedia)</a> eller <a href="https://en.wikipedia.org/wiki/Hyperbolic_function#Tanh">hyperbolisk tangent (Wikipedia)</a>.</li>
</ul>

<p><a name="gradientdescent"></a></p>
<ul>
  <li><strong>Gradient descent</strong> - <em>f√∂lja gradientens riktning</em> - en optimeringsteknik (generellt s√• optimerar vi funktioner som har h√∂gdimensionella indata). F√∂rst ber√§knas gradienten (motsvarar derivatan i endimensionella fallet). D√§refter tas ett litet steg i riktning mot brantaste sluttningen. Processen upprepas tills n√•got avslutningskriterium uppn√•tts.</li>
</ul>

<p><a name="representation"></a></p>
<ul>
  <li><strong>Representation</strong> - <em>representation</em> - i denna avhandling s√• syftar <em>representation</em> typiskt p√• en vektor som ber√§knats f√∂r att representera ett objekt s√•som ett ord, en bild, eller en mening. <a href="#ann">Neuronn√§t</a> ber√§knar en s√•dan vektorrepresentation vid varje lager <a href="#layer">layer</a>.</li>
</ul>

<p><a name="feature"></a></p>
<ul>
  <li><strong>Feature</strong> - <em>egenskap</em> - i m√•nga tekniker f√∂r maskininl√§rning s√• kr√§vs att man ber√§knar <em>features</em> och ger som indata till programmet. Dessa <em>features</em> eller <em>egenskaper</em> √§r ofta resultatet av en stor arbetsinsats, och ger en vy av datan som inte kan f√∂r√§ndras f√∂r att f√• systemet att fungera b√§ttre. I <a href="#ann">neuronn√§t</a> s√• l√§r sig systemet alla features som anv√§nds, i form av de inl√§rda representationerna.</li>
</ul>

<p><a name="objective"></a></p>
<ul>
  <li><strong>Objective</strong> - <em>felfunktion</em> - m√§ter hur bra modellen fungerar. Denna anv√§nds som kriterium f√∂r att optimera parametrarna i en modell (se <a href="#gradientdescent">gradient descent</a>). Felfunktionen √§r ofta formulerad genom ett avst√•ndsm√•tt som m√§ter skillnaden p√• utdatan fr√•n modellen j√§mf√∂rt med den √∂nskade utdatan som den ser ut i <a href="#trainingdata">tr√§ningsdatan</a>.</li>
</ul>

:ET