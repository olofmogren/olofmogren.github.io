<!doctype html>
<html class="no-js">
  <head>
      
      <meta charset="utf-8">
      <title>ACL 2016</title>
      <meta name="description" content="">
      <meta name="viewport" content="width=device-width">
      <link rel="stylesheet" href="/style.css">
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script>
        function showBibtex(event, text) {
          var el, x, y;

          el = document.getElementById('PopUp');
          if (window.event) {
            x = window.event.clientX + document.documentElement.scrollLeft
                + document.body.scrollLeft;
                y = window.event.clientY + document.documentElement.scrollTop +
                + document.body.scrollTop;
          }
          else {
            x = event.clientX + window.scrollX;
            y = event.clientY + window.scrollY;
          }
          x -= 2; y -= 2;
          y = y+15
          el.style.left = x + "px";
          el.style.top = y + "px";
          el.style.display = "block";
          document.getElementById('PopUpText').innerHTML = text;
       }
    </script>
  </head>
  <body class="home">

    <div class="bodydiv">
      <header id="header-waypoint">
        <nav id="stickyNav" class="sticky-nav">
          <div class="nametag">
            <div class="nametagsubdiv"><a href="http://mogren.one/" class="nametag"><h1 class="nametag">O<span style="font-size: 22px;">lof</span> M<span style="font-size: 22px;">ogren</span></h1></a></div>
            <div class="nametagsubscript"><span class="nametagsubscript">Senior machine learning researcher, PhD, at Research institutes of Sweden.</span></div>
          </div>
          <div id="groups" style="margin-right: 80px;">
            <div id="group1" style="z-index: 5;">
              <a href="/blog" style="color: #999;">Blog</a>
              <a href="/talks" style="color: #999;">Talks</a>
            </div>
            <div id="group2" style="z-index: 5;">
              <a href="/publications" style="color: #999;">Publications</a>
              <a href="/about" style="color: #999;">About</a>
            </div>
          </div>
        </nav>

      </header>

      <a class="github-ribbon" href="https://github.com/olofmogren">
        <img style="position: absolute; top: 0; right: 0; border: 0; z-index: 4;"
             src="/graphics/git-ribbon.png" alt="Fork me on GitHub">
      </a>

      <div id="PopUp" style="display: none; position: absolute; left: 100px; top: 50px; border: solid black 1px; padding: 10px; background-color: rgb(200,200,200); text-align: justify; font-size: 12px; min-width: 400px;">
        <pre id="PopUpText">TEXT</pre>
        <input name="close" type="button" value="Close" onclick="document.getElementById('PopUp').style.display = 'none';" />
      </div>

  <section>
<div class="inner">
<div class="block block-copy">


<h1>ACL 2016</h1>
<p><em>2016-08-22</em></p>


<figure style="float: right; max-width: 40%;">
<img src="/graphics/illustrations/2016-08-22/acl-logo54.png" alt="ACL 2016 in Berlin." style="max-width: 100%" />
<figcaption style="max-width: 100%;"></figcaption>
</figure>




<p>August 7-12, the 54th conference of the Association of Computational Linguistics (ACL) took place at the Humboldt University in Berlin, along with co-located tutorials and workshops. The conference was attended by roughly 1700 people, and hundreds of papers were presented during the sessions.</p>

<p>In this blog post, I intend to give an overview of some of the noteworthy papers
presented at ACL this year. The selection is based on my own taste,
in combination with my impressions from reading the papers
and attending the presentations. Feel free to give me your views
in the comment section in the bottom of the page.</p>

<p>Also see my blog posts about the <a href="http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html">Tutorial on Neural Machine Translation</a> and the <a href="http://mogren.one/blog/2016/08/11/representation-learning-for-nlp.html">First Workshop on Representation Learning for NLP</a>.</p>

<h2 id="monday">Monday</h2>

<p><strong>Generating Factoid Questions with RNNs</strong>, Iulian Vlad Serban; Alberto García-Durán; Caglar Gulcehre; Sungjin Ahn; Sarath Chandar; Aaron Courville; Yoshua Bengio</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1056.pdf">PDF, aclweb.org</a></p>

<p>This paper presents a large corpus of 30 million question-answer pairs,
designed to be used in the development of QA systems. The data is
synthetically created using factoid data from Freebase and a neural
model. The generated questions are evaluated and
compared to questions generated by a template-based method.</p>

<p>Iulian Serban gave a really nice presentation, illustrating a
somewhat creative solution to an interesting problem.</p>

<p><strong>Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</strong>, Minh-Thang Luong and Christopher D. Manning</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1100.pdf">PDF, aclweb.org</a></p>

<p>This paper presents a novel NMT model where out-of-vocabulary words are
treated as a character sequence. The authors report big improvements in BLEU scores
over models that handle OOV words by dictionary lookup, and claim that this
model produces translations with better quality of the OOV translations.</p>

<p>When the system comes across an OOV word, it uses a deep LSTM
<a href="https://en.wikipedia.org/wiki/Long_short-term_memory">(Wikipedia)</a>
trained on character level, always initialized with a zero state.
The final internal state of this character model
is used as the representation for this OOV word.
The reason for the zero initialization is to make the system simple;
the representation of all instances of one OOV word can be precomputed 
before using the word-based model.
The authors show that the embeddings created by the character-based
model are comparable to those learned by a normal word-based model.
This comes as a nice property of the learning; no
explicit learning signal is provided for this to happen.</p>

<p>When the word-based decoder produces an &lt;UNK&gt; token, the character based
decoder is used.
The approach is nice,
and does away with some of the issues associated with neural word-based
models, but still requires segmentation of the words as a preprocessing
step, something that a character-based model should be able to learn.
(Also see my blog post on
<a href="http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html">recent trends in neural machine translation</a>).</p>

<p><strong>Improving Neural Machine Translation Models with Monolingual Data</strong>, Sennrich, Haddow, and Birch</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1009.pdf">PDF, aclweb.org</a></p>

<p>Neural machine translation models can be viewed as language models
that are conditioned on an input sentence in the source language.
This work suggests that we can enhance the performance of the system,
by feeding it synthetical data. The authors propose to use a monolingual
training-corpus which they translate back from
the target language into the source language (using a baseline
NMT system trained only on parallel data), and then
train the model on this synthetic parallel data, mixed
with real parallel data written by human translators.
The method improves the performance by up to
three BLEU points. They also evaluate using other input
in the encoding part, but without much improvement over the
baseline. The result is interesting, as it allows for a kind of
semi-supervised approach to train NMT systems, and we have
the same conclusion that we are used to with neural models:
having more data is more important than having high-quality data.</p>

<p><strong>Together we stand: Siamese Networks for Similar Question Retrieval</strong>,
Arpita Das, Harish Yenala, Manoj Chinnakotla, and Manish Shrivastava</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1036.pdf">PDF, aclweb.org</a></p>

<p>This paper presents a convolutional neural network model that embeds posts
in community question answer systems. The model uses a twin layout
with tied weights and is trained by a contrastive loss function.
They manage to outperform existing methods based on translation models,
topic models, and some neural models.
The proposed method is a rather elegant way of learning similar representations
for semantically similar questions, a reasonable approach to find similar
posts in a discussion forum.</p>

<p>Our own paper, <strong>Assisting Discussion Forum Users using Deep Recurrent Neural Networks</strong>,
is related to this, but instead of convnets we use LSTMs to embed
forum posts. <a href="http://mogren.one/publications/2016/assisting/">Read more.</a></p>

<h2 id="tuesday">Tuesday</h2>

<p><strong>Neural Machine Translation of Rare Words with Subword Units</strong>, Rico Sennrich, Barry Haddow, and Alexandra Birch</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1162.pdf">PDF, aclweb.org</a></p>

<p>The authors present a neural machine translation model that works on
subword units to overcome the problem of words that are not part
of the vocabulary. NMT systems struggle with large vocabularies,
and in previous work, the handling of out-of-vocabulary words (OOV)
have been primitive at best. This solution builds a vocabulary using
the byte-pair encoding (BPE)
<a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">(Wikipedia)</a>
algorithm, segmenting words into n-grams. In the talk, Rico Sennrich
said that it can be viewed as a “character level model working on
compressed character data”. This is one of at least three different
papers that present work on the problem with rare words in NMT systems
at this years ACL. The model is nice, rare words need no special
treatment after the preprocessing of segmenting the input
data, and they present good results, improving over a baseline
(that handles rare words with a lexicon lookup) with roughly
1 BLEU point.
(Also see my blog post on
<a href="http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html">recent trends in neural machine translation</a>).</p>

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2016-08-22/hamilton-diachronic-embeddings.png" alt="Meaning of words that change over time." style="max-width: 100%" />
<figcaption>
Two-dimensional visualization of semantic change in English using SGNS vectors. 2 a, The word gay shifted from
meaning “cheerful” or “frolicsome” to referring to homosexuality. b, In the early 20th century broadcast referred to “casting
out seeds”; with the rise of television and radio its meaning shifted to “transmitting signals”. c, Awful underwent a process of
pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appalling” (Simpson et al., 1989).
</figcaption>
</figure>

<p><strong>Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change</strong>, William Hamilton, Jure Leskovec and Dan Jurfsky</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1141.pdf">PDF, aclweb.org</a></p>

<p>This work presents a nice application to use word embeddings
to track the changes of semantics over time, along with some
interesting observations.</p>

<p><strong>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</strong>, Junyoung Chung, Kyunghyun Cho, Yoshua Bengio</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1160.pdf">PDF, aclweb.org</a></p>

<p>This paper presents a model trained using subword-units as inputs and a
character sequence as output. The input is segmented using the byte-pair
encoding algorithm
as in Sennrich et.al. 2015.
The character-based decoder outperforms a decoder with subword-units.
Traditionally, translation systems work on word-level, even though a neural
model can suffer from large vocabulary sizes. This is why we see a number
of systems that try to remedy this by modelling subwords,
characters, or both.
(Also see my blog post on
<a href="http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html">recent trends in neural machine translation</a>).</p>

<h2 id="wednesday">Wednesday</h2>

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2016-08-22/joan-bresnan-lifetime-achievement-award.jpg" alt="Meaning of words that change over time." style="max-width: 100%" />
<figcaption>
Professor Joan Bresnan, giving the acceptance speech at ACL 2016.
</figcaption>
</figure>

<p><strong>ACL’s Lifetime achivement award 2016</strong> was given to linguistics professor Joan Bresnan of Stanford who gave a nice acceptance speech about her transition
from viewing natural language through the lens of formal grammars
to working with probabilistic methods that model linguistic phenomena.
Initially a PhD student under supervision of
Noam Chomsky <a href="https://en.wikipedia.org/wiki/Noam_Chomsky">(Wikipedia)</a>, she spent the first part of her academic life working
with grammatical formalisms, and in the 1970’s she helped to develop a theoretical
formal grammatical framework called
Lexical Functional Grammars, LFG
<a href="https://en.wikipedia.org/wiki/Lexical_functional_grammar">(Wikipedia)</a>.</p>

<p>In her talk she decribed how she some years ago
had a shock realizing that grammatical
rules may be inconsistent with each other.
With the availability of large amounts of computer-readable texts,
and with inspiration from artificial neural networks and visualizations
of quantitative data, she made
the jump from the garden [of linguistics] into the bush
[of data-driven research], as she phrased it.
One of the first publications after this transition was
<em>“Predicting the dative alternation”</em>
<a href="http://web.stanford.edu/~bresnan/CFI04.pdf">(PDF, web.stanford.edu)</a>.</p>

<p>See Professor Bresnan’s own writeup of the talk here:
<a href="http://www.stanford.edu/%7Ebresnan/jbLTA.pdf">(PDF, web.stanford.edu)</a>.</p>

<figure style="max-width: 40%; float: right; clear: both;">
<img src="/graphics/illustrations/2016-08-22/rl-dialogue.png" alt="Overview of the reinforcement learning system." style="max-width: 100%" />
<figcaption>
Schematic of the system framework. The three main system components dialogue policy,
dialogue embedding creation, and reward modelling based on user feedback.
Illustration from the paper. 
</figcaption>
</figure>

<p><strong>On-line active reward learning for policy optimization in spoken dialogue systems</strong> (outstanding paper), Su, Gasic, Mrksic, Barahona, Ultes, Vandyke, Wen, Young</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1230.pdf">PDF, aclweb.org</a></p>

<p>In a task-oriented dialogue system, the user has a clearly stated
goal, but training a policy-based system to provide useful responses
requres a viable measure of success.
This paper proposes a dialogue system based
on reinforcement learning where the reward function is learned
jointly with the dialogue policy. The reward function learns
to model how happy the user is with the interaction, and the
policy is used to generate responses.</p>

<p>The learned reward function operates on a fixed sized embedding of
the dialogue computed using
a neural sequence to sequence model with bidirectional
LSTM
<a href="https://en.wikipedia.org/wiki/Long_short-term_memory">(Wikipedia)</a>
units, trained as an autoencoder.</p>

<p>The poliicy optimization gets a small negative reward of -1 for each
turn in the dialogue, and a large positive reward if completion is
successful.
The dialogue success is modelled as a
Gaussian process (GP)
<a href="https://en.wikipedia.org/wiki/Gaussian_process">(Wikipedia)</a>,
and since getting explicit feedback from users can be costly and
time-consuming, users are asked to give such feedback only if the
GP model is uncertain. This feedback (coming from either the GP model,
in the cases when its uncertainty estimate is low, or directly
from the user) is then used as the reinforcement signal for
the policy learning.</p>

<p>Although the policy learning uses a variant of the
SARSA algorithm
<a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html">(Reinforcement Learning Algorithms at UNSW)</a>,
the paper is a bit thin on details about how the policy is formulated
and how its optimization works. The BiLSTM autoencoder
<a href="https://en.wikipedia.org/wiki/Autoencoder">(Wikipedia)</a>
is merely used to generate embeddings for the dialogue, while one
could imagine building on this and letting the neural model
take care of more of the interesting parts of the solution.
However, the paper is well written, the idea is nice, and
the presentation at ACL was very good!</p>

<p><strong>Thorough examination of CNN/Daily Mail reading comprehention task</strong> (outstanding paper), Danqi Chen, Jason Bolton, Chris Manning</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1223.pdf">PDF, aclweb.org</a></p>

<p>A nice paper showing two approaches to solve the CNN/Daily Mail
reading comprehension task. The authors show that their 
relatively simple baseline system with a feature-based
classifier beats the state-of-the-art system
by more than 5%, and suggest that harder reading comprehension
datasets are required. Interestingly, one day before this presentation,
two related datasets were presented at ACL,
the LAMBADA dataset from Denis Paperno et.al.
<a href="http://aclweb.org/anthology/P/P16/P16-1144.pdf">(PDF, aclweb.org)</a>
and the WikiReading dataset for language understanding on Wikipedia
from Daniel Hewlett et.al. at
<a href="http://research.google.com/">Google Research</a>
<a href="http://aclweb.org/anthology/P/P16/P16-1145.pdf">(PDF, aclweb.org)</a>.</p>

<p>The CNN/Daily Mail dataset contains
article text paired with questions based on bullet point
summaries from the source web pages. This paper also presents a neural
system with an attention mechanism, trained to output
an entity token, which beats the baseline with a small margin.
(But recall that the simple baseline beats the previous
state-of-the-art by a large margin)!</p>

<p>The paper is very well written, and the presentation was great.
It’s nice to see papers where some work has gone into creating
strong baselines.</p>

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2016-08-22/shrdlearn.png" alt="SHRDLEARN" style="max-width: 100%" />
<figcaption>
The SHRDLURN game: the objective
is to transform the start state into the goal state.
The human types in an utterance, and the computer
(which does not know the goal state) tries to interpret the
utterance and perform the corresponding action. The computer initially knows nothing
about the language, but through the human’s feedback, learns the human's language while making
progress towards the game goal. Picture from the paper by the original authors.
</figcaption>
</figure>

<p><strong>Learning language games through interaction</strong> (outstanding paper), Sida Wang, Percy Liang, Chris Manning</p>

<p><a href="http://aclweb.org/anthology/P/P16/P16-1224.pdf">PDF, aclweb.org</a></p>

<p>This paper presents a system that learns to communicate with a user
based on a collaborative game inspired by the classic AI
application SHRDLU
<a href="https://en.wikipedia.org/wiki/SHRDLU">(Wikipedia)</a>
(Winograd 1972).
The user gives commands in their own preferred language for the application
to carry out.</p>

<p>Initially, the computer does not know anything about the
language used, or about the goal of the interaction.</p>

<p>The work is both a study on how computers can learn language from a
collaborative interaction game, as well as on how people
act in such an environment. Through
Mechanical Turk <a href="https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk">(Wikipedia)</a>
users were given the task of interacting with the system.</p>

<p>When given a command, the computer uses what it has learned so far,
and proposes some possible actions, out of which which the user can accept one.</p>

<p>The language learning uses n-gram <a href="https://en.wikipedia.org/wiki/N-gram">(Wikipedia)</a>
features conjoined with tree-grams to represent the candidate parsing.
A log-linear model over the
logical forms <a href="https://en.wikipedia.org/wiki/Logical_form">(Wikipedia)</a>
learns to parse the utterances.</p>

<p>The presentation was entertaining, and the setting is interesting.
The solution is applied to a rather limited setting, where you can
only perform certain actions (and the actions are reasonably intuitive
for the users). However, the solution is nice, and the analysis gives some
insight both about strategies from human players, and what contributes
to a successful outcome of the game. The system is able to adapt to
different language use, but the best players also learn from the game and
adapt to make the learning go smoother, e.g. by writing more
consistently and lessen the use of synonyms. The setting is
certainly interesting, and you can see how this could be
useful for language-based interactions in some settings.</p>

<p><a href="http://shrdlurn.sidaw.xyz/">Live demo of the system (shrdlurn.sidaw.xyz)</a>.</p>

<h2 id="in-conclusion">In Conclusion</h2>

<p>All in all, lots of interesting research was presented at ACL 2016.
Around 1700 people attended the conference, up to seven sessions
ran in parallel, and the interest from inustry in this field is big.
Google, IBM, Amazon, Mendeley, and Maluuba were some of the companies
present with booths in the exhibition hall.</p>

<p>Already looking forward to next year’s conference in
<a href="https://www.aclweb.org/website/node/456">Vancouver</a>.</p>



<p>
<em>Olof Mogren</em><br />
</p>



</div>
</div>
</section>

<section>
<div class="inner">
<div class="block block-copy">
<div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//mogren-one.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>
</section>

<script id="dsq-count-scr" src="//mogren-one.disqus.com/count.js" async></script>



        <section class="foot">
          <strong>Olof Mogren</strong> Research institutes of Sweden<br /><br />
          <a href="https://www.linkedin.com/in/olof-mogren-5392b452" title="Follow me on LinkedIn"><img src="/graphics/logos/linkedin_logo_42x40-white.png" alt="LinkedIn" /></a>&nbsp;<!--
          //--><a href="https://twitter.com/olofmogren" title="Follow me on Twitter"><img src="/graphics/logos/twitter_logo_49x40-white.png" alt="Twitter" /></a>&nbsp;<!--
          <a href="https://flokk.no/i/9be9cd2a348d" title="Follow me on Diaspora"><img src="/graphics/logos/diaspora_logo_42x40-white.png" alt="Diaspora" /></a>&nbsp;
          //--><a href="http://mogren.one/feed.xml" title="Follow my posts with RSS."><img src="/graphics/logos/rss.svg" style="width: 40px; height: 40px;" alt="Atom/RSS Feed" /></a>
        </section>

      </div><!-- main content //-->
    </div>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11067757; 
var sc_invisible=1; 
var sc_security="5e553e07"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="free web stats"
href="http://statcounter.com/free-web-stats/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11067757/0/5e553e07/1/" alt="free
web stats"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
  </body>
</html>
