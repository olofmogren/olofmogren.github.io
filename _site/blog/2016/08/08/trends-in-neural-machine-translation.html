<!doctype html>
<html class="no-js">
  <head>
      
      <meta charset="utf-8">
      <title>Trends in Neural Machine Translation</title>
      <meta name="description" content="">
      <meta name="viewport" content="width=device-width">
      <link rel="stylesheet" href="/style.css">
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script>
        function showBibtex(event, text) {
          var el, x, y;

          el = document.getElementById('PopUp');
          if (window.event) {
            x = window.event.clientX + document.documentElement.scrollLeft
                + document.body.scrollLeft;
                y = window.event.clientY + document.documentElement.scrollTop +
                + document.body.scrollTop;
          }
          else {
            x = event.clientX + window.scrollX;
            y = event.clientY + window.scrollY;
          }
          x -= 2; y -= 2;
          y = y+15
          el.style.left = x + "px";
          el.style.top = y + "px";
          el.style.display = "block";
          document.getElementById('PopUpText').innerHTML = text;
       }
    </script>
  </head>
  <body class="home">

    <div class="bodydiv">
      <header id="header-waypoint">
        <nav id="stickyNav" class="sticky-nav">
          <div class="nametag">
            <div class="nametagsubdiv"><a href="http://mogren.one/" class="nametag"><h1 class="nametag">O<span style="font-size: 22px;">lof</span> M<span style="font-size: 22px;">ogren</span></h1></a></div>
            <div class="nametagsubscript"><span class="nametagsubscript">Senior machine learning researcher, PhD, at Research institutes of Sweden.</span></div>
          </div>
          <div id="groups" style="margin-right: 80px;">
            <div id="group1" style="z-index: 5;">
              <a href="/blog" style="color: #999;">Blog</a>
              <a href="/talks" style="color: #999;">Talks</a>
            </div>
            <div id="group2" style="z-index: 5;">
              <a href="/publications" style="color: #999;">Publications</a>
              <a href="/about" style="color: #999;">About</a>
            </div>
          </div>
        </nav>

      </header>

      <a class="github-ribbon" href="https://github.com/olofmogren">
        <img style="position: absolute; top: 0; right: 0; border: 0; z-index: 4;"
             src="/graphics/git-ribbon.png" alt="Fork me on GitHub">
      </a>

      <div id="PopUp" style="display: none; position: absolute; left: 100px; top: 50px; border: solid black 1px; padding: 10px; background-color: rgb(200,200,200); text-align: justify; font-size: 12px; min-width: 400px;">
        <pre id="PopUpText">TEXT</pre>
        <input name="close" type="button" value="Close" onclick="document.getElementById('PopUp').style.display = 'none';" />
      </div>

  <section>
<div class="inner">
<div class="block block-copy">


<h1>Trends in Neural Machine Translation</h1>
<p><em>2016-08-08</em></p>


<figure style="float: right; max-width: 40%;">
<img src="/graphics/illustrations/2016-08-08/manning-nmt-history.png" alt="History of machine translation systems. Slide by Christopher D. Manning." style="max-width: 100%" />
<figcaption style="max-width: 100%;">History of machine translation systems. Slide by Christopher D. Manning.</figcaption>
</figure>




<p>The following blog post was written as an attempt to summarize some
of the important things happening in the field of
neural machine translation (NMT). It was also a way of digesting
a great tutorial on NMT given Sunday, August 7 at ACL in Berlin
by some of the best people in the field: Christopher Manning,
Minh-Thang Luong, and Kyunghyun Cho.</p>

<p>Machine translation has gone through a number of stages in the last decades.
Phrase based statistical machine translation (SMT), the flavour
used in systems such as Google Translate have seen
little improvement over the last three years. 
Instead, people have been turning their heads towards
neural machine translation (NMT) systems, which
after being introduced seriously in 2014 have seen many refinements. 
These systems are also known as sequence-to-sequence models
or encoder-decoder networks, and were initially 
fairly simple neural network models made out
of two recurrent parts. Firstly, an encoder part taking an input sentence
in the source language and computing an internal representation,
and secondly the decoder part, a neural language model
trained to be good at assigning a high probability to a
well-formed sentence in the target language, which can be used to
generate sentences that sound very good. Letting the language
model be conditioned on the input turns it into a translation
model. (See <em>Sequence to Sequence Learning with Neural Networks</em>
Ilya Sutskever, Oriol Vinyals, Quoc V. Le. NIPS 2014, <a href="http://arxiv.org/abs/1409.3215">PDF, arXiv</a>).
These early NMT systems worked on word level, which means that
a word is seen as a symbol, so the input to the encoder is
a unique index for each unique word, and the decoder being
constrained to pick words from this vocabulary.</p>

<figure style="float: right; max-width: 40%; clear: right;">
<img src="/graphics/illustrations/2016-08-08/bahdanau-etal-alignment.png" style="max-width: 90%" />
<figcaption style="max-width: 90%">The attention mechanism produces an alignment between source
sentence and target sentence. Illustration from
<em>Neural Machine Translation by Jointly Learning to Translate and Align</em>
by Dzmitry Bahdanau, KyungHuyn Cho, and Yoshua Bengio. ICLR 2015.
<a href="https://arxiv.org/abs/1409.0473">(PDF, arXiv)</a>.
</figcaption>
</figure>

<p>These models worked well and got some promising scores in evaluations, but
they had some drawbacks.
Firstly, the longer the input sentence,
the more difficult for the encoder to capture all important information
in the internal fixed-size representation.
Secondly, they are practical only for use with
a fairly limited vocabulary size.</p>

<p>The former of the two problems were addressed in 2015, 
with the introduction of the attention models, allowing for the
decoder part to “attend” to different parts of the input while
generating the output. This is also used in multi-modal models
for tasks such as image captioning where the attention mechanism
allows the decoder to focus on different parts of the input image
as it generates the output text.</p>

<p>The latter of the two problems has previously been addressed
by letting the NMT system output special &lt;UNK&gt; tokens
for words that are out-of-vocabulary (OOV)
and post-processing the output by replacing this with the
correspondng word in the source sentence, or looking them up
in a dictionary
(See <em>“Addressing the Rare Word Problem in Neural Machine Translation”</em> by Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, Wojciech Zaremba. ACL 2015
<a href="https://arxiv.org/abs/1410.8206">PDF, arXiv</a> ).
This can result in the translated word being in the wrong inflection,
or (worse) the word might not be in the dictionary at all
(e.g. misspelled words).
Better handling of OOV terms has been the focus of some work
during 2016, and the topic of a couple of papers being presented
during ACL 2016, taking place this week in Berlin.</p>

<p>In <em>Neural Machine Translation of Rare Words with Subword Units</em> by
Rico Sennrich and Barry Haddow and Alexandra Birch from
University of Edinburg
<a href="http://aclweb.org/anthology/P/P16/P16-1162.pdf">(PDF, aclweb.org)</a>,
a system is proposed that work on subword units.
Words are segmented using the Byte Pair Encoding (BPE) algorithm
<a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">(read about this on Wikipedia)</a>
into subword units of different length, and a vocabulary is built
using frequent such units.
The method internally creates embeddings for these subword units,
something that has been criticized for making it lack
the ability of relating words to each other.
Rico Sennrich however argued during his talk that there is no reason why the
word boundaries would be the best unit to have embedded.
There are examples of composite words in one language
that translates into a sequence of words in another language.
The model is rather simple and elegant, and gets good BLEU scores
<a href="https://en.wikipedia.org/wiki/BLEU">(read more on BLEU on Wikipedia)</a>
translating between German and English, as well as Russian and
English.</p>

<figure style="float: left; max-width: 40%; clear: left;">
<img src="/graphics/illustrations/2016-08-08/luong-hybrid-nmt.png" style="max-width: 90%" />
<figcaption style="max-width: 90%">Hybrid word-character model for NMT. From Minh-Thang Luong's slides.</figcaption>
</figure>

<p>A second paper, by Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio from
New York University and Université de Montréal,
titled <em>A Character-level Decoder without Explicit Segmentation for Neural Machine Translation</em>
<a href="http://aclweb.org/anthology/P/P16/P16-1160.pdf">(PDF, aclweb.org)</a>,
presents a model that also uses a vocabulary generated
with BPE (see above) in combination with a <em>“bi-scale recurrent neural network”</em>,
a recurrent network with GRU cells on two different time-scales,
giving it a sense of hierarchy. The authors conclude that using the
BPE tokens in the encoding part, along with a pure character-based
decoding part, produces the best translations. The evaluation is made
on four different language pairs: English-German, English-Russian,
English-Czech, and English-Finnish.</p>

<p>In
<em>Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</em>
by Minh-Thang Luong and Christopher D. Manning from Stanford
<a href="http://aclweb.org/anthology/P/P16/P16-1100.pdf">(PDF, aclweb.org)</a>,
a model is presented that works as a normal word-based sequence-to-sequence
model, as long as you feed it words in the vocabulary.
When the model encounters an OOV term, the system employs a second sequence model
working on character level. This model computes a representation
for any word that is expressible in the given set of characters, and experimental
results show that the representations computed in this way share
many of the properties of neural word embeddings computed on
word-level
<a href="https://en.wikipedia.org/wiki/Word_embedding">(read more on Word embeddings on Wikipedia)</a>.
The system shows large improvements in BLEU scores,
especially when used with a small word vocabulary,
on the task of translating between Czech and English.</p>

<p>These are some of the more interesting presentations that I look
forward to during this year’s ACL.</p>




<p>
<em>Olof Mogren</em><br />
</p>



</div>
</div>
</section>

<section>
<div class="inner">
<div class="block block-copy">
<div id="disqus_thread"></div>
<script>

/**
 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//mogren-one.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>
</section>

<script id="dsq-count-scr" src="//mogren-one.disqus.com/count.js" async></script>



        <section class="foot">
          <strong>Olof Mogren</strong> Research institutes of Sweden<br /><br />
          <a href="https://www.linkedin.com/in/olof-mogren-5392b452" title="Follow me on LinkedIn"><img src="/graphics/logos/linkedin_logo_42x40-white.png" alt="LinkedIn" /></a>&nbsp;<!--
          //--><a href="https://twitter.com/olofmogren" title="Follow me on Twitter"><img src="/graphics/logos/twitter_logo_49x40-white.png" alt="Twitter" /></a>&nbsp;<!--
          <a href="https://flokk.no/i/9be9cd2a348d" title="Follow me on Diaspora"><img src="/graphics/logos/diaspora_logo_42x40-white.png" alt="Diaspora" /></a>&nbsp;
          //--><a href="http://mogren.one/feed.xml" title="Follow my posts with RSS."><img src="/graphics/logos/rss.svg" style="width: 40px; height: 40px;" alt="Atom/RSS Feed" /></a>
        </section>

      </div><!-- main content //-->
    </div>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11067757; 
var sc_invisible=1; 
var sc_security="5e553e07"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="free web stats"
href="http://statcounter.com/free-web-stats/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11067757/0/5e553e07/1/" alt="free
web stats"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
  </body>
</html>
