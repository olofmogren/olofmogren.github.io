<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title type="text" xml:lang="en"></title>
  <link type="application/atom+xml" href="http://mogren.one/feed.xml" rel="self"/>
   <link type="text" href="http://mogren.one" rel="alternate"/>
  <updated>2020-05-20T14:16:35+02:00</updated>
  <id>http://mogren.one</id>
  <author>
    <name>Olof Mogren</name>
  </author>
  <rights>Olof Mogren</rights>
  

  
  <entry>
    
    <title>Publication: </title>
    
    <link href="http://mogren.one/publications/2020/05/20/2019-06-19-character-based.html"/>
    <updated>2020-05-20T14:16:35+02:00</updated>
    <id>http://mogren.one/publications/2020/05/20/2019-06-19-character-based.html</id>
    <content type="html"></content>
  </entry>
  

  
  <entry>
    
    <title>Publication: Semantic segmentation of fashion images using feature pyramid networks</title>
    
    <link href="http://mogren.one/publications/2019/semantic/"/>
    <updated>2019-11-02T00:00:00+01:00</updated>
    <id>http://mogren.one/publications/2019/semantic/</id>
    <content type="html"> Our main objective is to facilitate fashion image analysis. We do this through semantic segmentation of fashion images, using both textural information and cues from shape and context, where target classes are clothing categories. Our main contributions are *state-of-the-art semantic segmentation of fashion images*, and *modest memory and GPU/CPU usage*.
</content>
  </entry>
  

  
  <entry>
    
    <title>Publication: Generative modelling of semantic segmentation data in the fashion domain</title>
    
    <link href="http://mogren.one/publications/2019/generative/"/>
    <updated>2019-11-02T00:00:00+01:00</updated>
    <id>http://mogren.one/publications/2019/generative/</id>
    <content type="html"> In this work, we propose a method to generatively model the joint distribution of images and corresponding semantic segmentation maps using generative adversarial networks. We extend the Style-GAN architecture by iteratively growing the network during training, to add new output channels that model the semantic segmentation maps. We train the proposed method on a large dataset of fashion images and our experimental evaluation shows that the model produces samples that are coherent and plausible with semantic segmentation maps that closely match the semantics in the image. </content>
  </entry>
  

  
  <entry>
    
    <title>lab: Several different projects related to machine learning awaits skilled students.</title>
    
    <link href="http://mogren.one/lab/2019/11/01/students-wanted.html"/>
    <updated>2019-11-01T00:00:00+01:00</updated>
    <id>http://mogren.one/lab/2019/11/01/students-wanted.html</id>
    <content type="html">Master students wanted for a number of different master thesis projects.</content>
  </entry>
  

  
  <entry>
    
    <title>lab: Text sumarization using transfer learning</title>
    
    <link href="http://mogren.one/lab/2019/06/15/risne-siitova.html"/>
    <updated>2019-06-15T00:00:00+02:00</updated>
    <id>http://mogren.one/lab/2019/06/15/risne-siitova.html</id>
    <content type="html">Text summarization using transfer learning: Extractive and abstractive summarization using BERT and GPT-2 on news and podcast data. In collaboration with Spotify.</content>
  </entry>
  

  
  <entry>
    
    <title>lab: Question Answering In Conversational Context</title>
    
    <link href="http://mogren.one/lab/2019/06/15/mondal-nadhan.html"/>
    <updated>2019-06-15T00:00:00+02:00</updated>
    <id>http://mogren.one/lab/2019/06/15/mondal-nadhan.html</id>
    <content type="html">Question Answering In Conversational Context, Using FlowQA and BERT for modelling conversations in QuAC.</content>
  </entry>
  

  
  <entry>
    
    <title>lab: Deep learning for fashion analysis</title>
    
    <link href="http://mogren.one/lab/2019/06/15/korneliusson.html"/>
    <updated>2019-06-15T00:00:00+02:00</updated>
    <id>http://mogren.one/lab/2019/06/15/korneliusson.html</id>
    <content type="html">Generative adversarial networks modelling joint distribution over images and segmentation masks. Applied to fashion data.</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Neural ordinary differential equations</title>
    
    <link href="http://mogren.one/talks/2019/01/31/neural-ordinary-differential-equations.html"/>
    <updated>2019-01-31T00:00:00+01:00</updated>
    <id>http://mogren.one/talks/2019/01/31/neural-ordinary-differential-equations.html</id>
    <content type="html"> Seminar presentation of &amp;ldquo;Neural ordinary differential equations&amp;rdquo; by Chen, et.al., receving best paper award at NeurIPS 2018. </content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Swedish symposium on deep learning</title>
    
    <link href="http://mogren.one/talks/2018/09/05/swedish-symposium-on-deep-learning.html"/>
    <updated>2018-09-05T00:00:00+02:00</updated>
    <id>http://mogren.one/talks/2018/09/05/swedish-symposium-on-deep-learning.html</id>
    <content type="html"> September 5-6, 2018, RISE AI is presenting work at the Swedish symposium on deep learning. We have a poster about our work on blood glucose prediction with confidence estimation, and an oral presentation about character-based recurrent neural networks for morphological transformations. Come and talk to us! </content>
  </entry>
  

  
  <entry>
    
    <title>Blog post: EMNLP 2017</title>
    
    <link href="http://mogren.one/blog/2017/09/13/emnlp.html"/>
    <updated>2017-09-13T00:00:00+02:00</updated>
    <id>http://mogren.one/blog/2017/09/13/emnlp.html</id>
    <content type="html">The EMNLP conference took place in Copenhagen in September 2017. In this blog post I share some observations that I made during the conference. These included subword-level models, multilingual NLP, language grounding, and inspiration from children.</content>
  </entry>
  

  
  <entry>
    
    <title>Publication: Character-based recurrent neural networks for morphological relational reasoning</title>
    
    <link href="http://mogren.one/publications/2017/character-based/"/>
    <updated>2017-09-07T00:00:00+02:00</updated>
    <id>http://mogren.one/publications/2017/character-based/</id>
    <content type="html"> Given a demo relation (a pair of word forms) and a query word, we devise a character-based recurrent neural network architecture using three separate encoders and a decoder, trained to predict the missing second form of the query word. Our results show that the exact form can be predicted for English with an accuracy of 94.7%. For Swedish, which has a more complex morphology with more inflectional patterns for nouns and verbs, the accuracy is 89.3%.</content>
  </entry>
  

  
  <entry>
    
    <title>lab: Blood Glucose Prediction for Type 1 Diabetes using Machine Learning</title>
    
    <link href="http://mogren.one/lab/2017/06/01/meijner-persson.html"/>
    <updated>2017-06-01T00:00:00+02:00</updated>
    <id>http://mogren.one/lab/2017/06/01/meijner-persson.html</id>
    <content type="html">Long Short-term Memory based models for blood glucose prediction with confidence level outputs.</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Can we trust AI: A talk at the science festival</title>
    
    <link href="http://mogren.one/talks/2017/05/14/can-we-trust-ai.html"/>
    <updated>2017-05-14T00:00:00+02:00</updated>
    <id>http://mogren.one/talks/2017/05/14/can-we-trust-ai.html</id>
    <content type="html">During the science festival in Gothenburg, we had a session discussing artificial intelligence. The theme for the whole festival was &amp;ldquo;trust&amp;rdquo;, so we naturally named our session &amp;ldquo;Can we trust AI&amp;rdquo;. I gave an introduction, and shared my view of some of the recent progress that has been made in AI and machine learning, and then we had four other speakers giving their views of current state of the art. Finally, I chaired a discussion session that was much appreciated with the audience. The room was filled, and many people came up to us afterwards and kept the discussion going. The other speakers were Annika Larsson from Autoliv, Ola Gustavsson from Dagens Nyheter, and Hans Salomonsson from Data Intelligence Sweden AB.</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Takeaways from NIPS: meta-learning and one-shot learning</title>
    
    <link href="http://mogren.one/talks/2017/02/02/nips-takeaways.html"/>
    <updated>2017-02-02T00:00:00+01:00</updated>
    <id>http://mogren.one/talks/2017/02/02/nips-takeaways.html</id>
    <content type="html">Before the representation learning revolution, hand-crafted features were a prerequisite for a successful application of most machine learning algorithms. Just like learned features have been massively successful in many applications, some recent work has shown that you can also automate the learning algorithms themselves. In this talk, I'll cover some of the related ideas presented at this year's NIPS conference.</content>
  </entry>
  

  
  <entry>
    
    <title>Publication: Named entity recognition in Swedish health records with character-based deep bidirectional LSTMs</title>
    
    <link href="http://mogren.one/publications/2016/ner-char-blstm/"/>
    <updated>2016-12-12T00:00:00+01:00</updated>
    <id>http://mogren.one/publications/2016/ner-char-blstm/</id>
    <content type="html">We propose an approach for named entity recognition in medical data, using a character-based deep bidirectional recurrent neural network. Such models can learn features and patterns based on the character sequence, and are not limited to a fixed vocabulary. This makes them very well suited for the NER task in the medical domain. Our experimental evaluation shows promising results, with a 60% improvement in F 1 score over the baseline, and our system generalizes well between different datasets.</content>
  </entry>
  

  
  <entry>
    
    <title>Publication: C-RNN-GAN: Continuous recurrent neural networks with adversarial training</title>
    
    <link href="http://mogren.one/publications/2016/c-rnn-gan/"/>
    <updated>2016-12-10T00:00:00+01:00</updated>
    <id>http://mogren.one/publications/2016/c-rnn-gan/</id>
    <content type="html">Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Deep Learning Guest Lecture</title>
    
    <link href="http://mogren.one/talks/2016/10/06/deep-learning-guest-lecture.html"/>
    <updated>2016-10-06T00:00:00+02:00</updated>
    <id>http://mogren.one/talks/2016/10/06/deep-learning-guest-lecture.html</id>
    <content type="html">&lt;p&gt; A motivational talk about deep artificial neural networks, given to the students in FFR135 (Artificial neural networks). I gave motivations for using deep architechtures, and to learn hierarchical representations for data. &lt;/p&gt;</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: Recent Advances in Neural Machine Translation</title>
    
    <link href="http://mogren.one/talks/2016/09/29/nmt.html"/>
    <updated>2016-09-29T00:00:00+02:00</updated>
    <id>http://mogren.one/talks/2016/09/29/nmt.html</id>
    <content type="html">&lt;p&gt;Neural models for machine translation was introduced seriously in 2014. With the introduction of attention models their performance improved to levels comparable to those of statistical phrase-based machine translation, the type of translation we are all  familiar with through servies like Google Translate.&lt;/p&gt; &lt;p&gt;However, the models have struggled with problems like limited vocabularies, the need of large amounts of data for training, and that they are expensive to train and use.&lt;/p&gt; &lt;p&gt;In the recent months, a number of papers have been published to remedy some of these issues. This includes techniques to battle the limited vocabulary problem, and of using monolingual data to improve the performance. As recently as Monday evening (Sept 26), Google uploaded a paper on their implementation of these ideas, where they claim performance on par with human translators, both counted in BLEU scores, and in human evaluations.&lt;/p&gt; &lt;p&gt;During this talk, I'll go through the ideas behind these recent papers.&lt;/p&gt;</content>
  </entry>
  

  
  <entry>
    
    <title>Talk: ACL overview</title>
    
    <link href="http://mogren.one/talks/2016/09/22/acl.html"/>
    <updated>2016-09-22T00:00:00+02:00</updated>
    <id>http://mogren.one/talks/2016/09/22/acl.html</id>
    <content type="html">Short.</content>
  </entry>
  

  
  <entry>
    
    <title>Blog post: ACL 2016</title>
    
    <link href="http://mogren.one/blog/2016/08/22/acl2016.html"/>
    <updated>2016-08-22T00:00:00+02:00</updated>
    <id>http://mogren.one/blog/2016/08/22/acl2016.html</id>
    <content type="html">August 7-12, the 54th conference of the Association of Computational Linguistics (ACL) took place at the Humboldt University in Berlin. This blog post contains a write-up of some of my favourite presentations during the conference.</content>
  </entry>
  

</feed>

