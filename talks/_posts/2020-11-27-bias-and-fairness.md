---
title: Social bias and fairness in NLP
layout: talks
tags:
 - default
box-bg-imgsrc: 
imgsrc: /graphics/mogren-2018-julia-sjoberg.png
imgalt: Social bias and fairness in NLP
longversion: "Learned continuous representations for language units was the first trembling steps of making neural networks useful for natural language processing (NLP), and promised a future with semantically rich representations for downstream solutions. NLP has now seen some of the progress that previously happened in image processing: the availability of increased computing power and the development of algorithms have allowed people to train larger models that perform better than ever. Such models also make it possible to use transfer learning for language tasks, thus leveraging large widely available datasets.
</p><p>
In 2016, Bolukbasi, et.al., presented their paper “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings”, shedding lights on some of the gender bias that was available in trained word embeddings at the time. Datasets obviously encode the social bias that surrounds us, and models trained on that data may expose the bias in their decisions. It is important to be aware of what information a learned system is basing its predictions on. Some solutions have been proposed to limit the expression of societal bias in NLP systems. These include techniques such as data augmentation and representation calibration. Similar approaches may also be relevant for privacy and disentangled representations. In this talk, we’ll discuss some of these issues, and go through some of the solutions that have been proposed recently.
</p>
<h3>References</h3>
<p>

<ul>
<li>
Bolukbasi, et.al., NeurIPS 2016, Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings
</li>
<li>
Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science 356(6334):183–186
</li>
<li>
Zhao, et.al, EMNLP 2018, Learning Gender-Neutral Word Embeddings
</li>
<li>
Sahlgren & Ohlsson, 2018, Gender Bias in Pretrained Swedish Embeddings
</li>
<li>
Kiela & Bottou, EMNLP 2014, Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics
</li>
<li>
Kågebäck, Mogren, Tahmasebi, Dubhashi, 2014, Extractive summarization using continuous vector space models
</li>
<li>
Zhao, et.al., NAACL 2018, Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods
</li>
<li>
Zhang, et.al., AIES 2018, Mitigating Unwanted Biases with Adversarial Learning
</li>
<li>
Sato, et.al., ACL 2019, Effective Adversarial Regularization for Neural Machine Translation
</li>
<li>
Wang, et.al., ICML 2019, Improving Neural Language Modeling via Adversarial Training
</li>
<li>
Martinsson, J., Listo Zec, E., Gillblad, D., Mogren, O. Adversarial representation learning for synthetic replacement of private attributes. <a href=\"https://arxiv.org/abs/2006.08039\">https://arxiv.org/abs/2006.08039</a>, 2020.
</li></ul>"
shortversion: "Learned continuous representations for language units was the first trembling steps of making neural networks useful for natural language processing (NLP), and promised a future with semantically rich representations for downstream solutions. NLP has now seen some of the progress that previously happened in image processing: the availability of increased computing power and the development of algorithms have allowed people to train larger models that perform better than ever. Such models also make it possible to use transfer learning for language tasks, thus leveraging large widely available datasets.

</p><p>
In 2016, Bolukbasi, et.al., presented their paper “Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings”, shedding lights on some of the gender bias that was available in trained word embeddings at the time. Datasets obviously encode the social bias that surrounds us, and models trained on that data may expose the bias in their decisions. It is important to be aware of what information a learned system is basing its predictions on. Some solutions have been proposed to limit the expression of societal bias in NLP systems. These include techniques such as data augmentation and representation calibration. Similar approaches may also be relevant for privacy and disentangled representations. In this talk, we’ll discuss some of these issues, and go through some of the solutions that have been proposed recently.
"

venue: GAIA Conference 2020
authors: Olof Mogren
bibtex: 
permalink:
pdf: /talks/slides/mogren-2020-11-27-bias-and-fairness.pdf
overwriteurl: 
externallink: http://conference.gaia.fish
redirect_from:
---
