---
title: Recent Advances in Neural Machine Translation
layout: posts
tags:
 - default
imgsrc: 
imgalt: 
longversion:
shortversion: "<p>Neural models for machine translation was introduced seriously in 2014. With the introduction of attention models their performance improved to levels comparable to those of statistical phrase-based machine translation, the type of translation we are all  familiar with through servies like Google Translate.</p><p>However, the models have struggled with problems like limited vocabularies, the need of large amounts of data for training, and that they are expensive to train and use.</p><p>In the recent months, a number of papers have been published to remedy some of these issues. This includes techniques to battle the limited vocabulary problem, and of using monolingual data to improve the performance. As recently as Monday evening (Sept 26), Google uploaded a paper on their implementation of these ideas, where they claim performance on par with human translators, both counted in BLEU scores, and in human evaluations.</p><p>During this talk, we'll go through the ideas behind these recent papers.</p><p> Place: EDIT-room 3364</p><p> Time: Thursday, September 29, 10:30</p>"

venue: Chalmers Machine Learning Seminars
authors: Olof Mogren
bibtex: 
permalink:
pdf: 
overwriteurl: /blog/2016/08/08/trends-in-neural-machine-translation.html
externallink: "http://www.cse.chalmers.se/research/lab/seminars/"
---

