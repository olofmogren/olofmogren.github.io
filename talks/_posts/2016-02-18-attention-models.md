---
title: Neural Attention Models
layout: posts
tags:
imgsrc: 
imgalt: 
longversion: jaha
ilongversion: "<p>In artificial neural networks, attention models allow the system to focus on certain parts of the input. This has shown to improve model accuracy in a number of applications. In image caption generation, attention models help to guide the model towards the parts of the image currently of interest. In neural machine translation, the attention mechanism gives the model an alignment of the words between the source sequence and the target sequence.  In this talk, we'll go through the basic ideas and workings of attention models, both for recurrent networks and for convolutional networks. In conclusion, we will see some recent papers that applies attention mechanisms to solve different tasks in natural language processing and computer vision.<p><em>Mentioned papers</em></p><ul><li><a href=\"http://arxiv.org/abs/1409.0473\" title=\"Bahdanau et.al.\">Bahdanau et.al., Neural Machine Translation by Jointly Learning to Align and Translate</a></li><li><a href=\"http://arxiv.org/abs/1502.03044\" title=\"Xu et.al.\">Xu et.al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li><li><a href=\"http://arxiv.org/abs/1602.03001\" title=\"Allamanis et.al.\">Allamanis et.al., A Convolutional Attention Network for Extreme Summarization of Source Code</a></li></ul><p><em>Other related papers</em></p><ul><li><a href=\"http://arxiv.org/abs/1502.04623\" title=\"Gregor et.al.\">Gregor et.al., DRAW: A Recurrent Neural Network For Image Generation</a></li><li><a href=\"http://arxiv.org/abs/1506.03340\" title=\"Hermann et.al.\">Hermann et.al., Teaching Machines to Read and Comprehend</a></li><li><a href=\"http://arxiv.org/abs/1406.1078\" title=\"Cho\">Cho, et.al., Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li></ul><p><em>Links</em></p><ul><li><a href=\"https://re-work.co/blog/deep-learning-ilya-sutskever-google-openai\" title=\"Ilya Sutskever Interview\">Interview with Ilya Sutskever</a></li></ul></p>"

shortversion: In artificial neural networks, attention models allow the system to focus on certain parts of the input. This has shown to improve model accuracy in a number of applications. In image caption generation, attention models help to guide the model towards the parts of the image currently of interest. In neural machine translation, the attention mechanism gives the model an alignment of the words between the source sequence and the target sequence.  In this talk, we'll go through the basic ideas and workings of attention models, both for recurrent networks and for convolutional networks. In conclusion, we will see some recent papers that applies attention mechanisms to solve different tasks in natural language processing and computer vision.

venue: Talk, Chalmers Machine Learning Seminars
authors: Olof Mogren
bibtex: 
permalink:
pdf: /talks/slides/mogren-2016-02-18-ml-seminar-attention.pdf
overwriteurl: 
externallink: http://www.cse.chalmers.se/research/lab/seminars/
---

