---
title: EMNLP 2017
layout: blogposts
tags:
 - 
imgsrc: /graphics/illustrations/2017-09-12/
imgalt: 
imgcaption: 

shortversion: 

venue: 
authors: Olof Mogren
permalink:
pdf: 
overwriteurl: 
---

Yesterday was the last day of the EMNLP conference (Empirical Methods in Natural Language Processing).
With this post, I hope to share some of the high-level observations I made during the conference
about trends in the area, or at least to gather my thoughts a bit after five days of intense
soaking of information.

<p style="text-align: center;">&gt;&gt; <a href="https://twitter.com/intent/tweet?text={{ "Interesting presentations at #acl2016berlin! @olofmogren" | url_encode }}&url={{"http://mogren.one"}}{{ page.url | url_encode }}" rel="nofollow" target="_blank" title="Click to Tweet this">Click to Tweet this</a> &lt;&lt;</p>

## Subword-level modeling

Throughout EMNLP, subword-level models was a recurring topic. One of the most attended workshops
was **SCLeM** (Subword and character-level methods), organized and chaired by Hinrich Sch&uuml;tze
and Yadollah Yaghoobzadeh, a workshop with an exciting line-up of invited speakers.

Subword-level models solve natural language tasks on the raw character-stream, without the
need to tokenize (split into word-tokens) the text.
This eliminates the source of error that is the tokenization, and lets the model work
with an open vocabulary (it can generalize to unseen words).
A neural subword-level model can learn where the boundaries are for chunks of
the text that carries meaning, and can get efficiency gains from not having to
consider distributions over large vocabularies in the output layers.
Subword models are de-facto standard in neural machine translation since late 2016,
and the idea is spreading to many other tasks.



<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2017-09-12/mikolov-future-work.jpg" alt="Thomaš Mikolov" style="max-width: 100%" />
<figcaption>
Thomaš Mikolov.
</figcaption>
</figure>

Thomaš Mikolov gave an thorough walk-through
on the history of
[neural language modelling, NLM (Wikipedia)](https://en.wikipedia.org/wiki/Language_model#Neural_language_models)
on subword-level.
He explained that working on word-level is suboptimal when there is character
variability, e.g. as in Czech.
Mikolov called the word-based language models introduced in 2007 a &ldquo;pseudo-solution&rdquo;.
In 2010, the RNN LMs were introduced, but the performance of these models working on
characters was not satisfactory, or they required too much time to train.
Still today, character-based RNN LMs perform slightly worse than their word-based counterparts,
but they are not limited to the vocabulary seen at training time.
Mikolov mentioned some of their recent papers on reducing computation in RNNs.
**Alternative structures for character-level RNNs**, Piotr Bojanowski, Armand Joulin, Tomáš Mikolov, (ICLR 2016), [PDF, openreview.net](https://openreview.net/pdf?id=wVqzL1ypocG0qV7mtLqm), and on training an RNN to learn when to update the weights
[(ICLR 2017)](https://arxiv.org/abs/1611.06188).

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2017-09-12/cho---what-is-a-sentence.png" alt="What is a sentence?" style="max-width: 100%" />
<figcaption>
Kyunghyun Cho started with a thought-provoking question.
</figcaption>
</figure>

Kyunghyun Cho started his talk with the question &ldquo;What is a sentence?&rdquo;
He explained that looking up
[&ldquo;sentence&rdquo; on Wikipedia](https://en.wikipedia.org/wiki/Sentence_(linguistics))
does not give you an answer to this question.
It gives you many pointers as to how a sentence *should be*, but not what it is.
In 2015, neural machine translation had improved to a point where to OOV tokens
had become a limiting factor.
Some solutions to this was
Sennrich et.al. (ACL 2016)
[PDF, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1162.pdf),
using byte-pair-encoding, BPE, to compute a subword vocabulary,
and
Luong and Manning (ACL 2016)
[PDF, aclwen.org](http://aclweb.org/anthology/P/P16/P16-1100.pdf),
using a hybrid approach with a character-based RNN as
fallback for OOV (out-of-vocabulary) terms.
These approaches still required the preprocessing step of tokenization.
Chung et al. (ACL 2016)
[PDF, aclweb.org](http://www.aclweb.org/anthology/P/P16/P16-1160.pdf)
proposed a model with a decoder working only with characters,
without any tokenization neccessary. The attention mechanism works perfectly,
even when the encoder works on subwords, and the decoder works with raw characters.
It consistently obtains results better than or equally good as the models using
subword units on both sides.

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2017-09-12/livescu.jpg" alt="Siamese convnets." style="max-width: 100%" />
<figcaption>
Training embeddings of different sound recordings of the same word into similar embeddings using convnets and siamese training.
</figcaption>
</figure>

Karen Livescu described her work on embedding speech into vector representations
using convnets, something that can be used for query-by-example, and can
easily learn intra-speaker variations.
These acousting embeddings can be trained jointly with textual embeddings,
by using a form of siamese training. She declared that the goal
is to do NLP directly on the speech signal, but that there is much left to do.

In the SCLeM poster sessions we presented our work on
[Character-based recurrent neural networks for morphological relational reasoning (mogren.one)](/publications/2017/character-based/),
where we learn how to represent morphological transformations of words,
and can transfer these transformations between words.
Our poster was in good company of 23 other posters with a range of related topics, from
morphology to embeddings of parts of Chinese characters.

The subword-level trend is an exciting development as it does away with one more
preprocessing step, and lets the model learn a larger part of the problem.
Learning as much as possible generally makes for robust NLP solutions.
In the workshop panel discussion, Sharon Goldwater declared that some bias is preferable in
NLP models because it can help reduce the dependency on large datasets, a concern
that can be motivated in some low-resource settings.
The splitting of words however, is often arbitrary, and the rules about where a space
*should be* in a character sequence differs between languages.
Subword level models are certainly something we will see more of in the future.

## Multilingual NLP

Besides machine translation being the topic of a large chunk of this year's papers,
multilingual work was definately a trend throughout the whole conference.
(Machine translation and multilinguality made up about a third of all submitted papers this year).

On Friday, Manaal Faruqui, Anders Søgaard, and Ivan Vulić gave a nice
tutorial on multilingual embeddings,
surveying the field of how to train word embeddings shareable across languages,
with the obvious benefit for working with low-resource languages.
Søgaard highlighted a useful resource being the Watchtower; a great resource, certainly
with a lot of religious stuff, but with contemporary language, and in plenty of languages.
&ldquo;We put it all on github with the blessing of Jehova's witnesses&rdquo;.


An example of multilingual (and multi-modal) NLP systems was presented by
{TODO: AUTHORS}
where image descriptions in English and German was embedded into the same embedding space
as the image itself, allowing for maulti-lingual image search.
{EXTRAPOLATE!}

## Inspiration from infants learning

Nando de Freitas gave an exciting keynote speech on Saturday morning, in which he began by saying he
didn't know much about language (something that may indicate that machine learning is
becoming such an intricate part of natural language processing that NLP conferences are keen on having
prominent machine learning researchers as keynote speakers even if language is not their main
field of interest). On his first slide, he showed a video of his daughter as a one-year-old
pulling lego blocks apart, and being noteably excited when she finally figured out how to do so.

Learning happening in context.

Learning to learn.

Grounded language learning with simulations and robot arms.

Sharon Goldwater also started her keynote with some motivation from infants learning,
but this time it was specifically about how one learns parts of words and words, respectively.

## Other exciting topics

Language grounding, NLP on speech

