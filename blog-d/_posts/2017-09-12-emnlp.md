---
title: EMNLP 2017
layout: blogposts
tags:
 - 
imgsrc: /graphics/illustrations/2017-09-12/
imgalt: 
imgcaption: 

shortversion: 

venue: 
authors: Olof Mogren
permalink:
pdf: 
overwriteurl: 
---

Yesterday was the last day of the EMNLP conference (Empirical Methods in Natural Language Processing).
With this post, I hope to share some of the high-level observations I made during the conference
about trends in the area, or at least to gather my thoughts a bit after five days of intense
soaking of information.

<p style="text-align: center;">&gt;&gt; <a href="https://twitter.com/intent/tweet?text={{ "Interesting presentations at #acl2016berlin! @olofmogren" | url_encode }}&url={{"http://mogren.one"}}{{ page.url | url_encode }}" rel="nofollow" target="_blank" title="Click to Tweet this">Click to Tweet this</a> &lt;&lt;</p>

## Subword-level modeling

Throughout EMNLP, subword-level models was a recurring topic. One of the most attended workshops
was **SCLeM** (Subword and character-level methods), organized and chaired by Hinrich Sch&uuml;tze
and Yadollah Yaghoobzadeh, a workshop with an exciting line-up of invited speakers.

Subword-level models solve natural language tasks on the raw character-stream, without the
need to tokenize (split into word-tokens) the text.
This eliminates the source of error that is the tokenization, and lets the model work
with an open vocabulary (it can generalize to unseen words).
A neural subword-level model can learn where the boundaries are for chunks of
the text that carries meaning, and can get efficiency gains from not having to
consider distributions over large vocabularies in the output layers.
Subword models are de-facto standard in neural machine translation since late 2016,
and the idea is spreading to many other tasks.


## Invited talk by Toma≈° Mikolov: Subword-level Information in NLP using Neural Networks

After a few introductory words by Hinrich Sch&uuml;tze,
this exciting workshop started with Thoma&shat; Mikolov, who gave an intriguing 
walk-through
on the history of language modelling (LM) using artificial neural networks
(neural language modelling, NLM) on subword-level.
Mikolov's motivation for working on word-level is that it is suboptimal when there is character
variability, e.g. as in Czech.
The journey started with early character-based feed-forward neural network (FFNN) models
which showed dissapointing performance (see e.g. Bengio, 2006).
These early models were not based on recurrent neural networks (RNN),
nowadays almost ubiquitous when modelling sequences of data.
Mikolov called the word-based language models introduced in 2007 a &ldquo;pseudo-solution&rdquo;.
In 2010, the RNN LMs were introduced, but the performance of these models working on
characters was not satisfactory, or they required too much time to train.
Still today, character-based RNN LMs perform slightly worse than their word-based counterparts,
but they are not limited to the vocabulary seen at training time.
A good in-between hack is to segment words into multi-character units, while retaining
the most frequent words as-is in the vocabulary.
Wrappping up, Mikolov mentioned fastText, the text classification framework recently
open-sourced by his team:
&ldquo;strong on analogy tasks, while being fast and
robust to typos and terms that are out of the vocabulary (OOV)&rdquo;.
Mikolov also mentioned some of their recent papers on reducing computation in RNNs
(ICLR 2016), and on traiing an RNN to learn when to update the weights
[(ICLR 2017)](https://arxiv.org/abs/1611.06188).

Noah Smith used up a lot of his time talking about brown clusters, a clustering technique
that is also used to create vector representations.
He spoke about learning to tokenize, and then with his orthographic
language embeddings.
In conclusion, he said tokens are broken in many ways, character models
can solve some of its problems, but you need to thinkk about the whole spectrum;
&ldquo;Don't think only character-level, don't think only word-level;
think holistically&rdquo;.


<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2017-09-12/cho---what-is-a-sentence.png" alt="What is a sentence?" style="max-width: 100%" />
<figcaption>
Kyunghyun Cho: What is a sentence?
</figcaption>
</figure>

Kyunghyun Cho started his talk with the question &ldquo;What is a sentence?&rdquo;
He explained that looking up
[&ldquo;sentence&rdquo; on Wikipedia](https://en.wikipedia.org/wiki/Sentence_(linguistics))
does not give you an answer to this question.
It gives you many pointers as to how a sentence *should be*, but not what it is.
The boundaries of words is perhaps something that humans consider important, but
is not necessarily ...
In 2015, neural machine translation had progressed to a point where to OOV tokens
started to become a limiting factor.
This in combination with the source of error
that any tokenizer is, called for a solution.
Rico Sennrich et.al.[citation!] proposed using a subword-level model, working on n-gram tokens
defined by the byte-pair ecoding approach.
Luong and Manning[CITATION] proposed a word-based solution with character-based back-off,
using a separate RNN to compute representations for OOVs using the character stream.
These approaches still required the preprocessing step of tokenization.
Chung et al., 2016 proposed a model with a decoder working only with characters,
without any tokenization neccessary. The attention mechanism works perfectly,
even when the encoder works on subwords, and the decoder works with raw characters.
It consistently obtains results better than or equally good as the models using
subword units on both sides.

In the SCLeM poster sessions we presented our work on
[Character-based recurrent neural networks for morphological relational reasoning (mogren.one)](/publications/2017/character-based/),
where we learn how to represent morphological transformations of words,
and can transfer these transformations between words.
Our poster was in good company of 23 other posters with a range of related topics, from
morphology to embeddings of parts of Chinese characters.

This is an exciting development as it takes us one step closer to learning as
much as possible through data, which generally makes for robust NLP solutions.
In the panel discussion, Sharon Goldwater declared that some bias is preferable in
NLP models because it can help reduce the dependency on large datasets, a concern
that can be motivated in some low-resource settings.

## Multilingual NLP

Besides machine translation being the topic of a large chunk of this year's papers,
the topic was definately a trend throughout the whole conference.
(Machine translation and multilinguality made up about a third of all submitted papers this year).

On Friday, Anders Soegaard and --- gave the tutorial on multilingual embeddings,
surveying the field of how to train word embeddings shareable across languages,
with the obvious benefit for working with low-resource languages.

An example of multilingual (and multi-modal) NLP systems was presented by
{TODO: AUTHORS}
where image descriptions in English and German was embedded into the same embedding space
as the image itself, allowing for maulti-lingual image search.
{EXTRAPOLATE!}

## Inspiration from infants learning

Nando de Freitas gave an exciting keynote speech on Saturday morning, in which he began by saying he
didn't know much about language (something that may indicate that machine learning is
becoming such an intricate part of natural language processing that NLP conferences are keen on having
prominent machine learning researchers as keynote speakers even if language is not their main
field of interest). On his first slide, he showed a video of his daughter as a one-year-old
pulling lego blocks apart, and being noteably excited when she finally figured out how to do so.

Learning happening in context.

Learning to learn.

Grounded language learning with simulations and robot arms.

Sharon Goldwater also started her keynote with some motivation from infants learning,
but this time it was specifically about how one learns parts of words and words, respectively.

## Other exciting topics

Language grounding, NLP on speech

