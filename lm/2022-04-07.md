---
layout: default
metatags: "<meta name=\"robots\" content=\"noindex,nofollow\" />"
---
Welcome to this week's Learning Machines seminar.

**Title:** Data augmentation for natural language processing /// Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling

**Speaker:** Kyunghyun Cho, NYU

**Abstract:** Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine translation, we propose to explicitly minimize the oversmoothing rate during training. We conduct a set of experiments to study the effect of the proposed regularization on both model distribution and decoding performance. We use a neural machine translation task as the testbed and consider three different datasets of varying size. Our experiments reveal three major findings. First, we can control the oversmoothing rate of the model by tuning the strength of the regularization. Second, by enhancing the oversmoothing loss contribution, the probability and the rank of <eos> token decrease heavily at positions where it is not supposed to be. Third, the proposed regularization impacts the outcome of beam search especially when a large beam is used. The degradation of translation quality (measured in BLEU) with a large beam significantly lessens with lower oversmoothing rate, but the degradation compared to smaller beam sizes remains to exist. From these observations, we conclude that the high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model.

**About the speaker:** Kyunghyun Cho is an associate professor of computer science and data science at New York University and CIFAR Fellow of Learning in Machines & Brains. He is also a senior director of frontier research at the Prescient Design team within Genentech Research & Early Development (gRED). He was a research scientist at Facebook AI Research from June 2017 to May 2020 and a postdoctoral fellow at University of Montreal until Summer 2015 under the supervision of Prof. Yoshua Bengio, after receiving PhD and MSc degrees from Aalto University April 2011 and April 2014, respectively, under the supervision of Prof. Juha Karhunen, Dr. Tapani Raiko and Dr. Alexander Ilin. He tries his best to find a balance among machine learning, natural language processing, and life, but almost always fails to do so.

**Location:** This is an online seminar. Connect using Zoom.

**Zoom link:** [https://rise.zoom.us/j/208117140](https://rise.zoom.us/j/208117140)

**Upcoming seminars:**

* 2022-04-21: Jacob Zwart, U.S. Geological Survey
* 2022-04-28: Thomas Schön, Uppsala University
* 2022-05-05: Fredrik Carlsson, RISE Research Institutes of Sweden
* 2022-05-12: Fredrik Lindsten, Linköping University

More information and coming seminars: [https://ri.se/lm-sem](https://ri.se/lm-sem)

-- The Learning Machines Team

