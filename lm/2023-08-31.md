---
layout: default
metatags: "<meta name=\"robots\" content=\"noindex,nofollow\" />"
---
 
Welcome to this week's Learning Machines seminar.

**Title:** From domain adaptation to federated learning

**Speaker:** Zahra Taghiyar Renan, Halmstad University

**Abstract:** In recent years, data-driven methods have received increasing attention across many fields. These methods are generally based on machine learning and learn predictive models from provided training samples. The models predict the labels of previously unseen samples or the value of a dependent variable. However, the generalization ability of the models to predict the label of the test samples is inherently connected to the assumption that training and test samples are generated by independent and identically distributed random variables, the IID assumption. ,
,
The dynamic nature of the real world may violate the mentioned assumption. This may result in a mismatch between samples available for training and future samples used for exploiting the models. Statistically, training and test samples may originate from different distributions. We call each of these data and the corresponding data distributions &quot;domains&quot;. To alleviate the differences between the domains, a solution is Domain Adaptation (DA); DA methods reduce the distance between diverse distributions. The scenario most often addressed by the literature is that DA deals with two domains: Source and Target. In this scenario, the source dataset is fully labeled. The goal is to adapt source and target domains to construct a generalizable model for the target domain using the labeled source samples. However, in order to achieve this objective, domain adaptation typically requires centralizing samples from all domains to facilitate the adaptation process. ,
,
Privacy considerations often prevent data owners (clients) from sharing their data entirely. Consequently, decentralized learning methods, such as federated learning, have gained attention as alternatives to centralized approaches. These decentralized methods enable collaborative learning to build a shared global model. Given that distributed domains often possess statistical heterogeneity, the global model&#x27;s effectiveness can be impacted. Theoretically, we illustrate how heterogeneity among clients influences the global model&#x27;s performance on each individual client. This highlights the necessity for Decentralized Domain Adaptation approaches.

**About the speaker:** Zahra (Nasrin) Taghiyarrenani is currently in her last year as a Ph.D. candidate at Halmstad University, where her specialization lies in the area of Domain Adaptation and Federated Learning. Through her studies, she, by proposing new Domain Adaptation and Federated Learning methods, has effectively addressed the problems within computer network security and Predictive Maintenance.

**Location:** RISE Gothenburg Office (Arvid Hedvalls Backe 4), or online using Zoom.

**Date:** 2023-08-31 15:00

**Zoom link:** [https://rise.zoom.us/j/208117140?pwd=SHFHbHY2VXFNSkt6NFF2WkFPQ3VnQT09](https://rise.zoom.us/j/208117140?pwd=SHFHbHY2VXFNSkt6NFF2WkFPQ3VnQT09)


This seminar will have an in-person presence at RISE Gothenburg Office (Arvid Hedvalls Backe 4). Make sure that you arrive in good time.


**Upcoming seminars:**

* 2023-09-07, 15:00: Adam Breitholtz, Chalmers University of Technology, **digital and physical: RISE Gothenburg Office (Arvid Hedvalls Backe 4)**
* 2023-09-14, 15:00: Virginia Smith, CMU
* 2023-09-21, 15:00: Nico Lang, University of Copenhagen, **digital and physical: RISE Lund Office (Scheelevägen 17)**
* 2023-09-28, 15:00: Johan Östman, AI Sweden, **digital and physical: RISE Gothenburg Office (Arvid Hedvalls backe 4)**
* All times are in CET.

More information and coming seminars: [https://ri.se/lm-sem](https://ri.se/lm-sem)

-- The Learning Machines Team

