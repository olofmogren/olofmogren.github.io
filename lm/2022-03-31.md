---
layout: default
metatags: "<meta name=\"robots\" content=\"noindex,nofollow\" />"
---
Welcome to this week's Learning Machines seminar.

**Title:** Linear Regions of Deep Neural Networks

**Speaker:** Martin Trimmel, Lund University

**Abstract:** Many of the most widely used neural network architectures make use of rectified linear activations (ReLU, i.e. f(x) = max(x, 0)) and are therefore piecewise linear functions. The maximal subsets of the input space on which the network function is linear are called linear regions. If we want to better understand ReLU networks, it may be beneficial to understand these regions. There is the common intuition that the number of linear regions of neural networks measures their expressivity. Therefore a lot of focus has been placed on trying to obtain estimates of this number. However, this number is staggeringly high: Even very small networks have many more linear regions than there are atoms in the universe (10^80). This number is also much larger than the number of points in the dataset. This raises the question of how representative the number of linear regions is for network performance and how information extracted from training samples passes on to the many linear regions free of data for successful generalisation to test data. Our approach differs from previous ones focused on counting in that it investigates the linear coefficients associated to the linear linear regions. We propose TropEx, a tropical algebra-based algorithm extracting linear terms of the network function. This allows us to compare the network function with an extracted tropical function that agrees with the original network around all training data points, but which has much fewer linear regions. We also use our algorithm to compare different network types from the perspective of linear regions and their coefficients.

**About the speaker:** Martin Trimmel has been a doctoral student at Lund University since October 2017 under the supervision of Professor Cristian Sminchisescu. Martin obtained a Master of Advanced Studies degree from the University of Cambridge and a BSc degree from the University of Edinburgh. He did both his Master’s and Bachelor’s degrees in mathematics, focusing on algebra and algebraic geometry during his time at Cambridge. During his undergraduate studies Martin also spent a semester at Université Toulouse-Jean-Jaurès in France. In his research, Martin is interested in using mathematics to obtain a better theoretical understanding of deep learning. In particular, his previous work has focused on the effect the activations have on the network function and its performance.

**Location:** This is an online seminar. Connect using Zoom.

**Zoom link:** [https://rise.zoom.us/j/208117140](https://rise.zoom.us/j/208117140)

**Upcoming seminars:**

* 2022-04-07: Kyunghyun Cho, NYU
* 2022-04-21: Jacob Zwart, U.S. Geological Survey
* 2022-04-28: Thomas Schön, Uppsala University
* 2022-05-05: Fredrik Carlsson, RISE Research Institutes of Sweden

More information and coming seminars: [https://ri.se/lm-sem](https://ri.se/lm-sem)

-- The Learning Machines Team

