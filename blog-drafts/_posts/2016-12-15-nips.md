---
title: NIPS 2016 impressions - unfinished draft
layout: blogposts
tags:
 - 
imgsrc: /graphics/illustrations/2016-12-15/nips-cake.png
imgalt: The machine learning cake by Yann LeCunn. It turned into a meme during NIPS 2016.
imgcaption: 

shortversion: Some impressions collected during NIPS 2016 in Barcelona.

venue: 
authors: Olof Mogren
permalink:
pdf: 
overwriteurl: 
---

In early December, the Conference on Neural Information Processing Systems (NIPS)
took place in Barcelona. This was the second time a place outside of north
america was hosting the event, and after growing agressively the last few
years, about 6.000 people attended this year's conference.

*Monday* started with tutorials. I attended the **Deep reinforcement learning**
tutorial, by Peter Abbeel and John Schulman, a thorough technical session
mainly about policy optimization, in the introduction of which
Peter Abbeel said that we'll cover the planned topics if we talk
fast enough. This was certainly the case, and I'm happy that I
already knew some of the concepts and ideas from before, something that
I think helped me digest the material much better. The tutorial took place
in one of the smaller rooms, but was still attended by almost 2000 people,
and the two speakers made good use of the three hours, covering techniques
ranging from the
*Cross-entropy method* (Szita, LÃ¶rincz, 2006) [(pdf, psu.edu)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.704.9726&rep=rep1&type=pdf)
to *Likelihood ratio policy gradient* such as *REINFORCE* (Williams, 1992) [(pdf, springer.com)](http://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf),
and different kinds of variance reduction techniques.

![OpenAI Universe](/graphics/illustrations/2016-12-15/openai-universe.png)

Abbeel concluded the tutorial with some current frontiers,
such as off-policy policy gradients, the utilization of auxilliary objectives,
and policy gradient methods for natural language,
and then went on to announce the all new
[OpenAI Universe](https://universe.openai.com/),
a platform for people to try out different RL algorithms on
thousands of different games and web browser interfaces.

The **Nuts and Bolts of Building Applications using Deep Learning tutorial**
by Andrew Ng gave some common sense suggestions on how to train and deploy
machine learning models in industry settings. There wasn't much technical
details.

Next up was the **Generative adversarial network (GAN) tutorial** by Ian Goodfellow.
He really made an effort to make the session a real tutorial, engaging
attendees in exercises, dispite having almost filled up the largest
hall, with a capacity of roughly five thousand people.
Goodfellow went through the basic ideas of adversarial training,
presented many tips and tricks for training GANs,
and presented some of the recent work in the field, with generated pictures
that look sharper and sharper for each paper that gets submitted
to arxiv.

In the middle of the tutorial J&uuml;rgen Schmidth&uum;ber went up to the
microphone and asked whether the original GAN ideas wasn't actually
equivalent to ideas presented in
&ldq;*Learning Factorial Codes by Predictability Minimization*&rdq;,
(Schmidth&uum;ber, 1992) [(abstract, pdf, mitpress.com)](http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.6.863#.WHT46WeYqwY).
J&uuml;rgen Schmidth&uum;ber is an important figure
in the development of the modern recurrent
neural networks as the second author of the original
&ldq;*Long Short-Term Memory*&rdq; paper (Hochreiter, SchidtH&uuml;ber, 1997)
[(abstract, pdf, mitpress.com)](http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735).
He has repeatedly claimed that his work has been overlooked
and that modern day deep learning researchers do not cite
original work as much as they should.
The interruption of the tutorial ended with Goodfellow calmly stating that
he disagreed that the ideas were the same, and that he'd told
this to Schmidth&uuml;ber in email correspondance.
He added that he didn't appreciate being interrupted when he was
trying to teach a topic to an audence.

The point was also made in the [original reviews (nips.cc)](http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips27/reviews/1384.html) of the original
&ldq;*Generative adversarial networks*&rdq; (Goodfellow et.al., 2014)
[(abstract, pdf, arxiv)](https://arxiv.org/abs/1406.2661).
The reviewer that pointed this out recommended that the paper would be rejected
(but it wasn't).
A lot of recent papers would not have been written would this have been the case.

Some papers on adversarial training that was presented at NIPS this year:
&ldq;*Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space*&rdq; (Nguyen, et.al., 2016)
[(abstract, pdf, arxiv)](https://arxiv.org/abs/1612.00005)
presents a model trained to generate images conditioned on
a class, or even on an image caption. They use Langevin sampling
and denoising autoencoders under the hood.
A related paper that is so far only on arxiv is
&ldq;*Image-to-Image Translation with Conditional Adversarial Networks*&rdq;
(Isola, et.al., 2016)
[(abstract, pdf, arxiv)](https://arxiv.org/abs/1611.07004)
presents a GAN that can take a drawing as input and create
a photorealistic image as output, or an aerial photograph as
input and a map as output, or an image segmentation to a photorealistic image.
The images printed in the paper looks really impressive.
&ldq;*Connecting Generative Adversarial Networks and Actor-Critic Methods*&rdq;
(Pfau, Vinyals, 2016)
[(abstract, pdf, arxiv)](https://arxiv.org/abs/1610.01945)
makes the connection from GANs to reinforcement learning.
&ldq;*Improved Techniques for Training GANs*&rdq;
(Salimans, et.al., 2016)
[(abstract, pdf, arxiv.org)](https://arxiv.org/abs/1606.03498)
goes through many of the tips and tricks presented during the GAN tutorial.
This includes replacing the normal generator objective function with minimizing
the difference of an internal representation within the disciminator
for generated data compared to real data.
Another thing one can do to improve results, is using labels where applicable,
letting the discriminator decide not only between real or fake, but also
outputting the right label.
To avoid that either the generator or the discriminator becomes too strong,
one can freeze training the strongest one.

TODO: Rocket AI

*Learning to learn by gradient descent by gradient descent*

*Learning to learn by gradient descent by gradient descent*
not only wins my award for the nicest play on words in the title,
it's also a really nice paper outlining how to use LSTMs to learn
the optimization strategy for an optimization problem.


TODO: meta-learning, RNN symposium.


