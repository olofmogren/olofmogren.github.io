---
title: 54th Conference of the ACL
layout: blogposts
tags:
 - NLP
 - ACL
imgsrc: /graphics/illustrations/2016-08-11/humboldt.jpg
imgalt: Humboldt University's main building in Berlin.
imgcaption: Humboldt University's main building in Berlin.

venue: 
authors: Olof Mogren
permalink:
pdf: 
overwriteurl: 
---

August 7-12, the 54th conference of the Association of Computational Linguistics (ACL) took place at the Humboldt University in Berlin, along with co-located tutorials and workshops. The conference was attended by roughly 1700 people, and hundreds of papers were presented during the sessions.

Also see my posts about the [Tutorial on Neural Machine Translation](http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html) and the [First Workshop on Representation Learning for NLP](http://mogren.one/blog/2016/08/11/representation-learning-for-nlp.html).

## Monday

(Perhaps change headings to be more topically related).

**Generating Factoid Questions with RNNs**, Iulian Vlad Serban; Alberto García-Durán; Caglar Gulcehre; Sungjin Ahn; Sarath Chandar; Aaron Courville; Yoshua Bengio

[PDF, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1100.pdf)

This paper presents a large corpus of 30 million question-answer pairs,
designed to be used in the development of QA systems. The data is
synthetically created using factoid data from Freebase and a neural
network architechture. The generated questions are evaluated and
compared to questions generated by a template-based method. 

* Nice presentation, interesting problem and somewhat creative solution.

**Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models**, Minh-Thang Luong and Christopher D. Manning

[PDf, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1100.pdf)

This paper presents a novel NMT model where out-of-vocabulary words are
treated as a character sequence. The authors report improvement in BLEU scores
over models that handle OOV words in some way, and claim that this
model produces translations with better quality of the OOV translations.

When the system comes across an OOV word, it uses a trained deep LSTM
on character level. This network is always initialized with a zero state.
The final internal state is used as the representation for this word.
The reason for the zero initialization is to make the system simple;
the representation of all instances of one OOV word can be precomputed 
before using the
word-based model.

When the word-based decoder produces an <UNK>, the character based
decoder is used. 

* How does this compare
  to the subtoken model presented by Bengio et.al.? The authors mention
  one paper doing this by Sennrich 2016, and claim that they do not
  manage to learn "relationships among words".?
* How is the character based model trained? Is there a regularization,
  or even main training objective that makes the representations
  similar between the two models?
  - NO. (asked Than Luong in person)
* Byte-pair algorithm starts with a one-character vocabulary, and then adds
  the most frequent two-gram to the vocabulary. 

**Improving Neural Machine Translation Models with Monolingual Data**, Sennrich, Haddow, and Birch

[PDf, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1009.pdf)

Neural machine translation models can be viewed as language models
that are conditioned on an input sentence in the source language.
This work suggests that we can enhance the performance of the system,
by feeding it synthetical data. The authors propose to use a monolingual
training-corpus which they translate back from
the target language into the source language (using a baseline
NMT system trained only on parallel data), and then
train the model on this synthetic parallel data, mixed
with real parallel data written by human translators.
The method improves the performance by up to
three BLEU points. They also evaluate using other input
in the encoding part, but without much improvement over the
baseline. The result is interesting, as it allows for a kind of
semi-supervised approach to train NMT systems, and we have
the same conclusion that we are used to with neural models:
having more data is more important than having high-quality data.

**Together we stand: Siamese Networks for Similar Question Retrieval**,
Arpita Das, Harish Yenala, Manoj Chinnakotla, and Manish Shrivastava

[PDF, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1036.pdf)

The authors propose a convolutional neural network to embed posts
in community question answer systems. The model uses a twin layout
with tied weights and is trained by a contrastive loss function.
They manage to outperform existing methods based on translation models,
topic models, and some neural models.
The proposed method is a rather elegant way of learning similar representations
for semantically similar questions, a reasonable approach to find similar
posts in a discussion forum.



## Tuesday


**Neural Machine Translation of Rare Words with Subword Units**, Rico Sennrich, Barry Haddow, and Alexandra Birch

[PDF, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1162.pdf)

The authors present a neural machine translation model that works on
subword units to overcome the problem of words that are not part
of the vocabulary. NMT systems struggle with large vocabularies,
and in previous work, the handling of out-of-vocabulary words (OOV)
have been primitive at best. This solution builds a vocabulary using
the [byte-pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding)
algorithm, segmenting words into n-grams. In the talk, Rico Sennrich
said that it can be viewed as a "character level model working on
compressed character data". This is one of at least three different
papers that work on the problem with rare words in NMT systems
at this years ACL. The model is nice, rare words need no special
treatment after the preprocessing of segmenting the input
data, and they present good results, improving over a baseline
(that handles rare words with a lexicon lookup) with roughly
1 BLEU point. Also see my blog post on
[recent trends in neural machine translation](http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html).


<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2016-08-12/hamilton-diachronic-embeddings.png" alt="Meaning of words that change over time." style="max-width: 90%" />
<figcaption>
Two-dimensional visualization of semantic change in English using SGNS vectors. 2 a, The word gay shifted from
meaning “cheerful” or “frolicsome” to referring to homosexuality. b, In the early 20th century broadcast referred to “casting
out seeds”; with the rise of television and radio its meaning shifted to “transmitting signals”. c, Awful underwent a process of
pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appalling” (Simpson et al., 1989).
</figcaption>
</figure>

**Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change**, William Hamilton, Jure Leskovec and Dan Jurfsky

This work presents a nice application to use word embeddings
to track the changes of semantics over time, along with some
interesting observations.


* Diachronic (historical) linguistics
* Qualitatively tendencies: "Grammaticalization" over time.
* Quantitatively tendencies:
* frequency, polysemy (factors that change over time)
* 4 languages
* Many embeddings.
* No evaluation of diachronic properties.
* Why do some words cahnge faster than others?
  - Mixed effect analysis.
* 5% of sentiment words changed polarity during last decade(s?)

**A Character-Level Decoder without Eplicit Segmentation for Neural Machine Translation**, Junyoung Chung, Kyunghyun Cho, Yoshua Bengio

This paper presents a model trained using subword-units as inputs and a
character sequence as output. The input is segmented using the byte-pair
encoding algorithm
as in Sennrich et.al. 2015.
The character-based decoder outperforms a decoder with subword-units.
Traditionally, translation systems work on word-level, even though a neural
model can suffer from large vocabulary sizes. This is why we see a number
of systems that try to remedy this, initially by subword-level modelling,
but now also on character-level. Also see my blog post on
[recent trends in neural machine translation](http://mogren.one/blog/2016/08/08/trends-in-neural-machine-translation.html).

## Wednesday

<figure style="max-width: 40%; float: right;">
<img src="/graphics/illustrations/2016-08-12/joan-bresnan-lifetime-achievement-award.jpg" alt="Meaning of words that change over time." style="max-width: 90%" />
<figcaption>
Professor Joan Bresnan, giving the acceptance speech at ACL 2016.
</figcaption>
</figure>


**ACL's Lifetime achivement award 2016** was given to linguistics professor Joan Bresnan of Stanford who gave a nice acceptance speech about her transition
from viewing natural language through the lens of formal grammars
to working with probabilistic methods that model linguistic phenomena.
Initially a PhD student under supervision of
Noam Chomskyv [(read more on Wikipedia)](https://en.wikipedia.org/wiki/Noam_Chomsky), she spent the first part of her academic life working
with grammatical formalisms, and in the 1970's she developed a theoretical
formal grammatical framework called
Lexical Functional Grammars, LFG [(read more on Wikipedia)](https://en.wikipedia.org/wiki/Lexical_functional_grammar).
At a point in her career, she had a shock realizing that grammatical
rules may be inconsistent with each other.
With the availability of large amounts of computer-readable texts,
and with inspiration from artificial neural networks and visualizations
of quantitative data, she made
the jump from the garden \[of linguistics\] into the bush
\[of data-driven research\], as she phrased it.
One of the first publications after this transition was
*"Predicting the dative alternation"*
[(PDF, web.stanford.edu)](http://web.stanford.edu/~bresnan/CFI04.pdf).

<figure style="max-width: 40%; float: right; clear: both;">
<img src="/graphics/illustrations/2016-08-12/rl-dialogue.png" alt="Overview of the reinforcement learning system." style="max-width: 90%" />
<figcaption>
Schematic of the system framework. The three main system components dialogue policy,
dialogue embedding creation, and reward modelling based on user feedback.
Illustration from the paper. 
</figcaption>
</figure>

**On-line active reward learning for policy optimization in spoken dialogue systems** (outstanding paper), Su, Gasic, Mrksic, Barahona, Ultes, Vandyke, Wen, Young

[PDF, aclweb.org](http://aclweb.org/anthology/P/P16/P16-1230.pdf)

This paper proposes a system where the reward function is learned
jointly with the dialogue policy. The reward function learns
to model how happy the user is with the interaction, and the
policy is used to generate responses. The underlying model
is a neural sequence to sequence model with bidirectional LSTM units.

* Pretty cool RL solution!
* Task-oriented dialogue system
* Success evaluation
* Goal is to find a suitable training system.
* Statistical spoken dialogue systems
* slide-photo.
* Per-turn penalty: -1
* Large reward if completion is successfulTypically requires pror knowledge of the task.
  - Simulated user
  - Paid users mturk
  x real users (have their own goal)
* How to learn from real users?
* Infer success (reward) directly from dialogues.
  - Train a reward estimator from data.
* User rating. (Noisy, difficult/costly to obtain)
* Propose a robust reward model on user rating.
* Learning a reward model.
  - Embedding function. Fixed length dialogue representation.
  - The leared reward model will then be used with reinforcement learning.
  - BLSTM encoder-decoder. As an autoencoder!
  - Mean squared error in reconstruction.
  - Gaussian process for success rating. Measurement of the uncertainty.
    + RBF kernel. Noise term. Affects the uncertainty. More noise -> less certain predictions.
  - ( I think that the gaussian process selects training examples?)
* Then the reward system is used in the SDS.
* Off-line RNN, Subj, Online GP
* All reached > 85% after 500 dialogues.
* Onlie GP is more robust than subj.
* Online GP only needs 150 dialogues forsomething.
* Conclusion slide on photo.
* Pretty cool!


**Thorough examination of CNN/Daily Mail reading comprehention task** (outstanding paper), Danqi Chen, Jason Bolton, Chris Manning

Comments:

* *Nice to implement a strong baseline. Both methods were actually interesting.*
* *Inspiring analysis, but did not follow entirely in the end; what are the differenct categories?*

* Reading comprehention data: need more
* In the CNN/Daily Mail, each article have a number of bullet points. Pick one, and black out one word (an entity, I think). Problem will be to predict the word.
* System 1: entity centric classifier
  - a symbolic feature vector for each entity.
* System 2: end-to-end neural system.
  - GRU
* Difference to attentive reader:
  - Bilinear attention
  - Remove redundant layer efore prediction
  - Predict among entities only, not all words.
* Conclusions:
  - Simple models sometimes just work. Neural nets are great for learning semantic matches.
  - The dataset is noisy and we have almost hst the capcacity: not hard enough for reasoning an dinference.
  - Future:
    + Leverage these datasets to solve more realistic RC tasks.
    + Complex models?
    + More datasets coming up: WikiReading, LAMBADA SQuAD


**Learning language games through interaction** (outstanding paper), Sida Wang, Percy Liang, Chris Manning

* *Pretty entertaining! Nice presentation*
* Wish list:
  - Interactive learning, adapt to users, handle special domains and low resouce
* Interactive language game.
  - Human player, knows goal, knows language
  - Computer player, does not know the goal, doesnot understaand language
    + need to learn language
* ShrdLURN
  - user writes command.
  - Is given some candidate actions; accepts one.
  - Semantic parsing
    + start from smallest size, score them with a model (log-linear model with features)
    + adagrad
    + features uni, bi, skipgrams, treegrams
* Players on mturk.
* Players adapt, starting to use only one term.
* Learning works fairly well, especially for top players.
* Cooperative communication
* Pragmatics model
  - Pragmatics helps for the top players
* Interactive learning is good for usability.
  - Feedback mechanism -> less likely to be stuck.
  - Should introduce learning to normal usage.
  - Good for low-resource languages
  - Learn from the actual distribution


| President's talk | Amusing at times. Nice overview on 25 years of MT |
